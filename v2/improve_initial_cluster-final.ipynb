{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb17d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0864b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(624, 1000)\n",
      "    586        384    87    751    81       262   697   581   364   383  ...  \\\n",
      "0  52.5  29.600000  65.4  11.20  22.5  3.580000  15.3  21.8  2.95  25.1  ...   \n",
      "1  63.6  42.800000  68.7  12.00  27.3  4.620000  20.2  27.7  3.27  31.9  ...   \n",
      "2  47.6   1.505021  75.8  10.20  32.6  3.310000  16.2  20.5  5.22  28.8  ...   \n",
      "3  59.2  40.100000  62.1  14.00  27.2  3.300000  18.9  30.9  5.48  26.5  ...   \n",
      "4  40.9  37.500000  74.7   4.61  32.9  1.510252  16.9  24.3  1.56  30.0  ...   \n",
      "\n",
      "     884   634    29   172   263    45   741   378        906    840  \n",
      "0  3.050  14.3  5.92  6.76  11.3  13.6  1.99  64.1  32.300000  12.70  \n",
      "1  1.660  18.3  6.83  8.31  15.7  20.9  2.31  47.6  30.400000   8.12  \n",
      "2  0.625  15.6  9.65  7.63  15.8  19.2  1.86  43.5  23.800000   8.85  \n",
      "3  3.580  16.1  7.15  6.30  11.3  19.6  2.15  54.8  31.100000  10.80  \n",
      "4  1.170  15.9  9.72  5.24  14.4  18.5  2.15  42.1   1.510252   9.81  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv(\"../data/vpufs_reduced_features.csv\")\n",
    "\n",
    "data = data.dropna(axis=1)\n",
    "data = data.loc[:, ~data.columns.duplicated()]\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "data = data.dropna()\n",
    "\n",
    "# 3. Extract features and labels\n",
    "labels = data.iloc[:, -1].values\n",
    "features_df = data.drop(data.columns[-1], axis=1)\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d86f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def normalized_euclidean_similarity(X):\n",
    "    \"\"\" Compute (1 - normalized Euclidean distance) similarity matrix \"\"\"\n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # Euclidean distances\n",
    "    distances = cdist(X_scaled, X_scaled, metric='euclidean')\n",
    "    # Normalize distances to [0, 1]\n",
    "    max_dist = distances.max()\n",
    "    normalized_dist = distances / max_dist\n",
    "    # Convert to similarity\n",
    "    similarity = 1 - normalized_dist\n",
    "    return similarity\n",
    "\n",
    "def build_clusters_with_overlap(similarity_matrix, r=0.9):\n",
    "    \"\"\" \n",
    "    Identify neighbors based on similarity threshold\n",
    "    ALLOWS samples to appear in multiple clusters for better merging\n",
    "    \"\"\"\n",
    "    n_samples = similarity_matrix.shape[0]\n",
    "    clusters = []\n",
    "    densities = []\n",
    "    \n",
    "    # For each sample, create a cluster with all its similar neighbors\n",
    "    for i in range(n_samples):\n",
    "        # Find neighbors based on similarity threshold\n",
    "        neighbors = np.where(similarity_matrix[i] >= r)[0]\n",
    "        \n",
    "        if len(neighbors) > 0:\n",
    "            clusters.append(neighbors.tolist())\n",
    "            densities.append(len(neighbors))\n",
    "    \n",
    "    return clusters, densities, similarity_matrix\n",
    "\n",
    "# def merge_clusters_by_similarity(clusters, similarity_matrix, threshold=0.7):\n",
    "#     \"\"\"\n",
    "#     Merge clusters based on center-to-center similarity\n",
    "#     \"\"\"\n",
    "#     print(f\"Starting with {len(clusters)} clusters\")\n",
    "    \n",
    "#     # Calculate cluster centers (most central sample in each cluster)\n",
    "#     centers = []\n",
    "#     for cluster in clusters:\n",
    "#         if not cluster:\n",
    "#             centers.append(-1)  # Mark empty clusters\n",
    "#             continue\n",
    "            \n",
    "#         # For each cluster, find the most central sample\n",
    "#         best_center = -1\n",
    "#         best_avg_sim = -1\n",
    "        \n",
    "#         for sample in cluster:\n",
    "#             # Average similarity to other samples in cluster\n",
    "#             if len(cluster) > 1:\n",
    "#                 avg_sim = np.mean([similarity_matrix[sample, other] for other in cluster])\n",
    "#             else:\n",
    "#                 avg_sim = 1.0  # Single-sample cluster\n",
    "                \n",
    "#             if avg_sim > best_avg_sim:\n",
    "#                 best_avg_sim = avg_sim\n",
    "#                 best_center = sample\n",
    "                \n",
    "#         centers.append(best_center)\n",
    "    \n",
    "#     # Convert clusters to sets for easier operations\n",
    "#     cluster_sets = [set(cluster) for cluster in clusters]\n",
    "    \n",
    "#     # Keep track of which clusters have been merged into others\n",
    "#     merged_into = [-1] * len(cluster_sets)  # -1 means not merged\n",
    "    \n",
    "#     # Check every pair of clusters\n",
    "#     # print(\"\\n--- Merging by Center Similarity ---\")\n",
    "#     merges_performed = 0\n",
    "    \n",
    "#     while True:\n",
    "#         best_sim = threshold\n",
    "#         best_i, best_j = -1, -1\n",
    "        \n",
    "#         # Find the most similar pair of clusters\n",
    "#         for i in range(len(cluster_sets)):\n",
    "#             if merged_into[i] != -1 or centers[i] == -1:  # Skip if merged or empty\n",
    "#                 continue\n",
    "                \n",
    "#             for j in range(i+1, len(cluster_sets)):\n",
    "#                 if merged_into[j] != -1 or centers[j] == -1:  # Skip if merged or empty\n",
    "#                     continue\n",
    "                \n",
    "#                 # Calculate similarity between centers\n",
    "#                 center_sim = similarity_matrix[centers[i], centers[j]]\n",
    "                \n",
    "#                 if center_sim > best_sim:\n",
    "#                     best_sim = center_sim\n",
    "#                     best_i, best_j = i, j\n",
    "        \n",
    "#         # If no pair found above threshold, we're done\n",
    "#         if best_i == -1:\n",
    "#             break\n",
    "\n",
    "        \n",
    "#         cluster_sets[best_i].update(cluster_sets[best_j])  # Add all elements from j to i\n",
    "#         merged_into[best_j] = best_i  # Mark j as merged into i\n",
    "#         merges_performed += 1\n",
    "\n",
    "        \n",
    "#     print(f\"Performed {merges_performed} similarity-based merges\")\n",
    "    \n",
    "#     # Collect final clusters (those not merged into others)\n",
    "#     final_clusters = []\n",
    "#     for i, merged_status in enumerate(merged_into):\n",
    "#         if merged_status == -1 and centers[i] != -1:  # Not merged into another and not empty\n",
    "#             final_clusters.append(list(cluster_sets[i]))\n",
    "    \n",
    "    \n",
    "#     # Sort by size\n",
    "#     final_clusters.sort(key=len, reverse=True)\n",
    "    \n",
    "    \n",
    "#     return final_clusters\n",
    "\n",
    "\n",
    "def merge_clusters_by_similarity(clusters, similarity_matrix, threshold=0.7, handle_small_clusters=True):\n",
    "    \"\"\"\n",
    "    Merge clusters based on center-to-center similarity and handle small clusters\n",
    "    \"\"\"\n",
    "    print(f\"Starting with {len(clusters)} clusters\")\n",
    "    \n",
    "    # Calculate cluster centers (most central sample in each cluster)\n",
    "    centers = []\n",
    "    for cluster in clusters:\n",
    "        if not cluster:\n",
    "            centers.append(-1)  # Mark empty clusters\n",
    "            continue\n",
    "            \n",
    "        # For each cluster, find the most central sample\n",
    "        best_center = -1\n",
    "        best_avg_sim = -1\n",
    "        \n",
    "        for sample in cluster:\n",
    "            # Average similarity to other samples in cluster\n",
    "            if len(cluster) > 1:\n",
    "                avg_sim = np.mean([similarity_matrix[sample, other] for other in cluster])\n",
    "            else:\n",
    "                avg_sim = 1.0  # Single-sample cluster\n",
    "                \n",
    "            if avg_sim > best_avg_sim:\n",
    "                best_avg_sim = avg_sim\n",
    "                best_center = sample\n",
    "                \n",
    "        centers.append(best_center)\n",
    "    \n",
    "    # Convert clusters to sets for easier operations\n",
    "    cluster_sets = [set(cluster) for cluster in clusters]\n",
    "    \n",
    "    # Keep track of which clusters have been merged into others\n",
    "    merged_into = [-1] * len(cluster_sets)  # -1 means not merged\n",
    "    \n",
    "    # First phase: merge by center similarity (with recursive propagation)\n",
    "    merges_performed = 0\n",
    "    \n",
    "    # This set will track which clusters need to be checked again after a merge\n",
    "    clusters_to_check = set(range(len(cluster_sets)))\n",
    "    \n",
    "    while clusters_to_check:\n",
    "        # Get a cluster to check\n",
    "        i = clusters_to_check.pop()\n",
    "        \n",
    "        # Skip if this cluster was merged into another\n",
    "        if merged_into[i] != -1 or centers[i] == -1:\n",
    "            continue\n",
    "            \n",
    "        # Find most similar cluster to merge with\n",
    "        best_sim = threshold\n",
    "        best_j = -1\n",
    "        \n",
    "        for j in range(len(cluster_sets)):\n",
    "            if i == j or merged_into[j] != -1 or centers[j] == -1:\n",
    "                continue\n",
    "                \n",
    "            # Calculate similarity between centers\n",
    "            center_sim = similarity_matrix[centers[i], centers[j]]\n",
    "            \n",
    "            if center_sim > best_sim:\n",
    "                best_sim = center_sim\n",
    "                best_j = j\n",
    "        \n",
    "        # If found a cluster to merge with\n",
    "        if best_j != -1:\n",
    "            # Merge clusters\n",
    "            cluster_sets[i].update(cluster_sets[best_j])\n",
    "            merged_into[best_j] = i\n",
    "            \n",
    "            # Add this cluster back to check set since it changed\n",
    "            clusters_to_check.add(i)\n",
    "            \n",
    "            # Update center for the merged cluster\n",
    "            if len(cluster_sets[i]) > 0:\n",
    "                best_center = -1\n",
    "                best_avg_sim = -1\n",
    "                \n",
    "                for sample in cluster_sets[i]:\n",
    "                    avg_sim = np.mean([similarity_matrix[sample, other] for other in cluster_sets[i]])\n",
    "                    if avg_sim > best_avg_sim:\n",
    "                        best_avg_sim = avg_sim\n",
    "                        best_center = sample\n",
    "                \n",
    "                centers[i] = best_center\n",
    "                \n",
    "            merges_performed += 1\n",
    "    \n",
    "    print(f\"Performed {merges_performed} recursive similarity-based merges\")\n",
    "    \n",
    "    # Second phase: handle small clusters\n",
    "    if handle_small_clusters:\n",
    "        small_cluster_threshold = 3  # Consider clusters with <= 3 samples as small\n",
    "        \n",
    "        # Identify small clusters\n",
    "        small_cluster_indices = []\n",
    "        for i in range(len(cluster_sets)):\n",
    "            if merged_into[i] == -1 and centers[i] != -1 and len(cluster_sets[i]) <= small_cluster_threshold:\n",
    "                small_cluster_indices.append(i)\n",
    "        \n",
    "        print(f\"Found {len(small_cluster_indices)} small clusters to potentially merge\")\n",
    "        \n",
    "        # Try to merge each small cluster into its most similar larger cluster\n",
    "        additional_merges = 0\n",
    "        for i in small_cluster_indices:\n",
    "            if merged_into[i] != -1:  # Skip if already merged\n",
    "                continue\n",
    "                \n",
    "            best_sim = 0.3  # Lower threshold for small clusters\n",
    "            best_j = -1\n",
    "            \n",
    "            # Find most similar larger cluster\n",
    "            for j in range(len(cluster_sets)):\n",
    "                if merged_into[j] != -1 or centers[j] == -1 or i == j:\n",
    "                    continue\n",
    "                    \n",
    "                if len(cluster_sets[j]) <= small_cluster_threshold:\n",
    "                    continue  # Skip other small clusters\n",
    "                \n",
    "                # Calculate average similarity between this small cluster and the larger one\n",
    "                avg_sim = 0\n",
    "                count = 0\n",
    "                for sample_i in cluster_sets[i]:\n",
    "                    for sample_j in cluster_sets[j]:\n",
    "                        avg_sim += similarity_matrix[sample_i, sample_j]\n",
    "                        count += 1\n",
    "                \n",
    "                if count > 0:\n",
    "                    avg_sim /= count\n",
    "                    \n",
    "                    if avg_sim > best_sim:\n",
    "                        best_sim = avg_sim\n",
    "                        best_j = j\n",
    "            \n",
    "            # Merge if found a good match\n",
    "            if best_j != -1:\n",
    "                cluster_sets[best_j].update(cluster_sets[i])\n",
    "                merged_into[i] = best_j\n",
    "                additional_merges += 1\n",
    "        \n",
    "        print(f\"Performed {additional_merges} additional merges for small clusters\")\n",
    "    \n",
    "    # Collect final clusters (those not merged into others)\n",
    "    final_clusters = []\n",
    "    for i, merged_status in enumerate(merged_into):\n",
    "        if merged_status == -1 and centers[i] != -1:  # Not merged into another and not empty\n",
    "            final_clusters.append(list(cluster_sets[i]))\n",
    "    \n",
    "    # Sort by size\n",
    "    final_clusters.sort(key=len, reverse=True)\n",
    "    \n",
    "    print(f\"Final number of clusters: {len(final_clusters)}\")\n",
    "    \n",
    "    return final_clusters\n",
    "\n",
    "def remove_duplicates(clusters, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Ensure each sample appears in only one cluster - the one where it has highest similarity\n",
    "    \"\"\"\n",
    "    n_samples = similarity_matrix.shape[0]\n",
    "    \n",
    "    # Track which cluster each sample is assigned to\n",
    "    sample_to_cluster = [-1] * n_samples\n",
    "    sample_similarities = [0.0] * n_samples\n",
    "    \n",
    "    # Process each cluster\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        for sample in cluster:\n",
    "            # Calculate average similarity to this cluster\n",
    "            cluster_sim = np.mean([similarity_matrix[sample, other] for other in cluster])\n",
    "            \n",
    "            # If not yet assigned or better similarity, assign to this cluster\n",
    "            if sample_to_cluster[sample] == -1 or cluster_sim > sample_similarities[sample]:\n",
    "                sample_to_cluster[sample] = cluster_idx\n",
    "                sample_similarities[sample] = cluster_sim\n",
    "    \n",
    "    # Create deduplicated clusters\n",
    "    deduplicated = [[] for _ in range(len(clusters))]\n",
    "\n",
    "  \n",
    "\n",
    "    # Assign each sample to its best cluster\n",
    "    for sample, cluster_idx in enumerate(sample_to_cluster):\n",
    "        if cluster_idx != -1:  # Skip unassigned samples\n",
    "            deduplicated[cluster_idx].append(sample)\n",
    "    \n",
    "    # Remove empty clusters\n",
    "    deduplicated = [cluster for cluster in deduplicated if cluster]\n",
    "    \n",
    "    # Sort by size\n",
    "    deduplicated.sort(key=len, reverse=True)\n",
    "    \n",
    "    return deduplicated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43520e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_pipeline(data: pd.DataFrame, labels=None, k=1000, r=0.9, merge_threshold=0.7):\n",
    "    print(\"Step 2: Computing similarity matrix...\")\n",
    "    similarity = normalized_euclidean_similarity(data)\n",
    "    \n",
    "    print(\"Step 3: Identifying neighbors and densities...\")\n",
    "    clusters, densities, similarity = build_clusters_with_overlap(similarity, r=r)\n",
    "\n",
    "    # # 🚩 Print initial clusters before merging\n",
    "    # print(f\"\\n🔹 Initial clusters (before merging): {len(clusters)}\")\n",
    "    # for i, cluster in enumerate(clusters):\n",
    "    #     print(f\"Cluster {i} (size={len(cluster)}): {cluster}\")\n",
    "    \n",
    "    print(\"\\nStep 4: Merging similar clusters...\")\n",
    "    merged = merge_clusters_by_similarity(clusters, similarity, threshold=merge_threshold)\n",
    "\n",
    "    # 🚩 Print clusters after merging but before deduplication\n",
    "    # print(f\"\\n🔸 Merged clusters (before deduplication): {len(merged)}\")\n",
    "    # for i, cluster in enumerate(merged):\n",
    "    #     print(f\"Cluster {i} (size={len(cluster)}): {cluster}\")\n",
    "    \n",
    "    print(\"\\nStep 5: Removing duplicates...\")\n",
    "    final_clusters = remove_duplicates(merged, similarity)\n",
    "\n",
    "    # 🚩 Print final deduplicated clusters\n",
    "    print(f\"\\n✅ Final clusters (after deduplication): {len(final_clusters)}\")\n",
    "    for i, cluster in enumerate(final_clusters):\n",
    "        print(f\"Cluster {i} (size={len(cluster)}): {cluster}\")\n",
    "\n",
    "    # Final cluster densities\n",
    "    final_densities = [len(cluster) for cluster in final_clusters]\n",
    "    \n",
    "    return final_clusters, final_densities, similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b5cedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Computing similarity matrix...\n",
      "Step 3: Identifying neighbors and densities...\n",
      "\n",
      "Step 4: Merging similar clusters...\n",
      "Starting with 624 clusters\n",
      "Performed 136 recursive similarity-based merges\n",
      "Found 478 small clusters to potentially merge\n",
      "Performed 477 additional merges for small clusters\n",
      "Final number of clusters: 11\n",
      "\n",
      "Step 5: Removing duplicates...\n",
      "\n",
      "✅ Final clusters (after deduplication): 11\n",
      "Cluster 0 (size=139): [30, 65, 85, 93, 94, 98, 100, 113, 115, 116, 119, 124, 144, 153, 155, 160, 161, 167, 176, 177, 178, 182, 184, 186, 192, 193, 194, 195, 206, 210, 221, 223, 225, 237, 242, 246, 248, 249, 259, 265, 273, 276, 297, 349, 362, 422, 423, 430, 431, 433, 434, 437, 438, 440, 442, 455, 469, 470, 472, 473, 474, 475, 476, 477, 478, 484, 485, 486, 487, 489, 490, 491, 492, 494, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 512, 514, 519, 520, 523, 524, 525, 528, 529, 530, 532, 533, 534, 536, 537, 538, 540, 542, 544, 545, 546, 547, 548, 549, 550, 551, 554, 555, 557, 558, 559, 560, 561, 562, 565, 572, 573, 579, 580, 582, 583, 584, 593, 598, 606, 611, 612, 618, 619, 621, 622, 623]\n",
      "Cluster 1 (size=107): [2, 13, 23, 29, 31, 42, 50, 55, 58, 61, 66, 69, 72, 82, 84, 87, 95, 96, 104, 107, 125, 128, 129, 154, 156, 159, 163, 285, 288, 295, 311, 312, 314, 316, 317, 319, 321, 322, 323, 326, 327, 328, 330, 331, 332, 334, 336, 337, 338, 339, 341, 342, 344, 345, 346, 347, 348, 350, 353, 354, 355, 356, 357, 360, 367, 368, 370, 371, 373, 375, 376, 378, 379, 380, 382, 383, 385, 386, 388, 391, 396, 397, 398, 399, 400, 401, 402, 403, 404, 406, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417, 419, 421, 424, 425, 426, 427, 428]\n",
      "Cluster 2 (size=78): [17, 20, 91, 101, 171, 189, 197, 198, 199, 200, 201, 202, 205, 207, 208, 209, 211, 212, 214, 215, 216, 217, 218, 219, 220, 222, 224, 226, 227, 228, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 243, 244, 245, 247, 250, 251, 257, 258, 260, 262, 263, 264, 267, 268, 269, 271, 272, 274, 439, 480, 493, 510, 517, 564, 567, 568, 570, 571, 574, 575, 576, 577, 581, 586, 587, 588, 589, 599]\n",
      "Cluster 3 (size=69): [33, 34, 37, 45, 81, 109, 157, 162, 266, 279, 281, 282, 283, 284, 286, 287, 289, 290, 291, 292, 293, 294, 296, 298, 299, 300, 302, 303, 304, 305, 306, 307, 308, 309, 310, 315, 320, 343, 351, 359, 363, 364, 365, 369, 377, 384, 390, 392, 393, 394, 395, 405, 429, 594, 595, 596, 597, 600, 601, 602, 603, 604, 605, 607, 608, 609, 615, 616, 617]\n",
      "Cluster 4 (size=66): [140, 170, 175, 183, 185, 187, 191, 196, 213, 229, 230, 277, 324, 325, 340, 361, 411, 432, 435, 436, 441, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 479, 481, 482, 495, 507, 511, 518, 521, 522, 526, 535, 539, 553, 563, 566, 585, 590, 591, 592, 610, 613]\n",
      "Cluster 5 (size=54): [0, 1, 3, 4, 5, 6, 8, 10, 11, 12, 25, 35, 38, 40, 48, 49, 52, 53, 57, 59, 60, 63, 71, 73, 75, 76, 83, 88, 90, 102, 103, 105, 114, 118, 121, 122, 130, 133, 135, 137, 139, 145, 149, 152, 158, 168, 318, 333, 335, 374, 387, 389, 420, 620]\n",
      "Cluster 6 (size=40): [21, 39, 51, 62, 64, 67, 70, 77, 80, 86, 99, 110, 132, 143, 165, 166, 172, 173, 174, 180, 181, 188, 204, 252, 253, 255, 256, 270, 278, 280, 488, 515, 516, 527, 531, 552, 556, 569, 578, 614]\n",
      "Cluster 7 (size=32): [9, 14, 15, 16, 18, 27, 32, 36, 56, 68, 108, 112, 120, 123, 126, 127, 134, 136, 142, 147, 150, 151, 169, 301, 313, 329, 352, 358, 366, 372, 381, 418]\n",
      "Cluster 8 (size=30): [7, 19, 24, 28, 41, 43, 44, 46, 47, 54, 74, 78, 79, 89, 92, 97, 106, 111, 117, 131, 138, 141, 146, 148, 164, 179, 254, 261, 275, 467]\n",
      "Cluster 9 (size=8): [22, 190, 203, 471, 483, 513, 541, 543]\n",
      "Cluster 10 (size=1): [26]\n"
     ]
    }
   ],
   "source": [
    "# Run the revised pipeline\n",
    "clusters, densities, similarity = run_pipeline(features_df, labels=labels, k=1000, r=0.87, merge_threshold=0.84)\n",
    "\n",
    "\n",
    "# print(\"\\nFinal clusters after deduplication:\")\n",
    "# # Display the top 15 clusters by size\n",
    "# for i in range(min(15, len(clusters))):\n",
    "#     print(f\"Cluster {i}: Size = {densities[i]}\")\n",
    "#     print(f\"Samples: {clusters[i]}\")\n",
    "#     print(\"-----------\")\n",
    "\n",
    "# # Also print the total number of samples in all clusters\n",
    "# total_samples = sum(densities)\n",
    "# print(f\"\\nTotal samples in all clusters: {total_samples}\")\n",
    "# print(f\"Number of clusters: {len(clusters)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2769c1",
   "metadata": {},
   "source": [
    "# Test to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995a655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring 83 parameter combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Computing similarity matrix...\n",
      "Step 3: Identifying neighbors and densities...\n",
      "\n",
      "Step 4: Merging similar clusters...\n",
      "Starting with 624 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/83 [00:01<02:19,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed 613 recursive similarity-based merges\n",
      "Found 10 small clusters to potentially merge\n",
      "Performed 7 additional merges for small clusters\n",
      "Final number of clusters: 4\n",
      "\n",
      "Step 5: Removing duplicates...\n",
      "\n",
      "✅ Final clusters (after deduplication): 4\n",
      "Cluster 0 (size=621): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623]\n",
      "Cluster 1 (size=1): [26]\n",
      "Cluster 2 (size=1): [164]\n",
      "Cluster 3 (size=1): [334]\n",
      "New best parameters found: r=0.9, merge_threshold=0.55, clusters=4\n",
      "Step 2: Computing similarity matrix...\n",
      "Step 3: Identifying neighbors and densities...\n",
      "\n",
      "Step 4: Merging similar clusters...\n",
      "Starting with 624 clusters\n"
     ]
    }
   ],
   "source": [
    "def explore_parameter_space(features_df, labels=None, k=1000):\n",
    "    \"\"\"\n",
    "    Explore different combinations of r and merge_threshold parameters\n",
    "    to find optimal settings for clustering.\n",
    "    \n",
    "    Args:\n",
    "        features_df: DataFrame containing features\n",
    "        labels: Optional array of true labels for evaluation\n",
    "        k: Number of features to select\n",
    "        \n",
    "    Returns:\n",
    "        best_params: Dictionary with best parameters found\n",
    "        all_results: DataFrame with all results for further analysis\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Define parameter ranges to explore\n",
    "    r_values = [0.95, 0.92, 0.90, 0.87, 0.85, 0.82, 0.80, 0.77, 0.75, 0.72, 0.70, 0.65, 0.60]\n",
    "    merge_values = [0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, 0.35]\n",
    "    \n",
    "    # Create combination pairs for more focused exploration\n",
    "    # We don't need to try all combinations - higher r works better with lower merge threshold\n",
    "    param_pairs = []\n",
    "    \n",
    "    # Add all combinations where r and merge_threshold sum is between 1.3 and 1.7\n",
    "    # This focuses on the most promising region of parameter space\n",
    "    for r in r_values:\n",
    "        for m in merge_values:\n",
    "            if 1.3 <= r + m <= 1.7:\n",
    "                param_pairs.append((r, m))\n",
    "    \n",
    "    # Add a few extreme combinations to better understand the parameter space\n",
    "    param_pairs.extend([\n",
    "        (0.95, 0.80),  # Very strict clustering\n",
    "        (0.60, 0.40),  # Very loose clustering\n",
    "        (0.90, 0.40),  # High initial threshold, aggressive merging\n",
    "        (0.70, 0.80)   # Low initial threshold, conservative merging\n",
    "    ])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    param_pairs = list(set(param_pairs))\n",
    "    \n",
    "    # Prepare results collection\n",
    "    results = []\n",
    "    best_cluster_count = float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    print(f\"Exploring {len(param_pairs)} parameter combinations...\")\n",
    "    \n",
    "    # Run clustering with each parameter combination\n",
    "    for r, merge_threshold in tqdm(param_pairs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run the pipeline with current parameters\n",
    "        clusters, densities, similarity = run_pipeline(\n",
    "            features_df, \n",
    "            labels=labels, \n",
    "            k=k, \n",
    "            r=r, \n",
    "            merge_threshold=merge_threshold,\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # Calculate cluster statistics\n",
    "        num_clusters = len(clusters)\n",
    "        total_samples = sum(densities)\n",
    "        \n",
    "        # Calculate average and median cluster size\n",
    "        avg_cluster_size = total_samples / num_clusters if num_clusters > 0 else 0\n",
    "        median_cluster_size = sorted(densities)[len(densities)//2] if densities else 0\n",
    "        \n",
    "        # Calculate size of largest and smallest clusters\n",
    "        largest_cluster = max(densities) if densities else 0\n",
    "        smallest_cluster = min(densities) if densities else 0\n",
    "        \n",
    "        # Calculate distribution stats - how many clusters of different sizes\n",
    "        small_clusters = sum(1 for size in densities if size <= 3)\n",
    "        medium_clusters = sum(1 for size in densities if 3 < size <= 10)\n",
    "        large_clusters = sum(1 for size in densities if size > 10)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'r': r,\n",
    "            'merge_threshold': merge_threshold,\n",
    "            'num_clusters': num_clusters,\n",
    "            'total_samples': total_samples,\n",
    "            'avg_cluster_size': avg_cluster_size,\n",
    "            'median_cluster_size': median_cluster_size,\n",
    "            'largest_cluster': largest_cluster,\n",
    "            'smallest_cluster': smallest_cluster,\n",
    "            'small_clusters': small_clusters,\n",
    "            'medium_clusters': medium_clusters,\n",
    "            'large_clusters': large_clusters,\n",
    "            'elapsed_time': elapsed_time\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Check if this is the best parameter set so far\n",
    "        # We want a reasonable number of clusters (not too many, not too few)\n",
    "        ideal_cluster_range = (20, 50)  # Adjust this range based on your preferences\n",
    "        \n",
    "        # If number of clusters is in the ideal range and better than previous best\n",
    "        if ideal_cluster_range[0] <= num_clusters <= ideal_cluster_range[1]:\n",
    "            if abs(num_clusters - sum(ideal_cluster_range)/2) < abs(best_cluster_count - sum(ideal_cluster_range)/2):\n",
    "                best_cluster_count = num_clusters\n",
    "                best_params = {'r': r, 'merge_threshold': merge_threshold}\n",
    "                print(f\"New best parameters found: r={r}, merge_threshold={merge_threshold}, clusters={num_clusters}\")\n",
    "        \n",
    "        # If we haven't found anything in the ideal range, prefer fewer clusters\n",
    "        elif best_cluster_count > ideal_cluster_range[1] and num_clusters < best_cluster_count:\n",
    "            best_cluster_count = num_clusters\n",
    "            best_params = {'r': r, 'merge_threshold': merge_threshold}\n",
    "            print(f\"New best parameters found: r={r}, merge_threshold={merge_threshold}, clusters={num_clusters}\")\n",
    "    \n",
    "    # Convert results to DataFrame for analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by number of clusters\n",
    "    results_df = results_df.sort_values('num_clusters')\n",
    "    \n",
    "    # Print summary of best parameter sets\n",
    "    print(\"\\n===== PARAMETER EXPLORATION RESULTS =====\")\n",
    "    print(f\"Best parameters found: r={best_params['r']}, merge_threshold={best_params['merge_threshold']}\")\n",
    "    print(f\"Number of clusters with best parameters: {best_cluster_count}\")\n",
    "    \n",
    "    # Display top 5 parameter combinations with fewest clusters\n",
    "    print(\"\\nTop 5 parameter combinations with fewest clusters:\")\n",
    "    top_5_fewest = results_df.head(5)\n",
    "    for _, row in top_5_fewest.iterrows():\n",
    "        print(f\"r={row['r']}, merge_threshold={row['merge_threshold']}: {row['num_clusters']} clusters\")\n",
    "    \n",
    "    # Display parameter combinations with 20-50 clusters\n",
    "    ideal_results = results_df[(results_df['num_clusters'] >= 20) & (results_df['num_clusters'] <= 50)]\n",
    "    if not ideal_results.empty:\n",
    "        print(\"\\nParameter combinations with 20-50 clusters:\")\n",
    "        for _, row in ideal_results.iterrows():\n",
    "            print(f\"r={row['r']}, merge_threshold={row['merge_threshold']}: {row['num_clusters']} clusters\")\n",
    "    \n",
    "    # Create a pivot table to visualize the parameter space\n",
    "    print(\"\\nParameter space visualization (number of clusters):\")\n",
    "    pivot_table = results_df.pivot_table(\n",
    "        index='r', \n",
    "        columns='merge_threshold', \n",
    "        values='num_clusters',\n",
    "        aggfunc='first'  # In case of duplicates\n",
    "    )\n",
    "    print(pivot_table)\n",
    "    \n",
    "    return best_params, results_df\n",
    "\n",
    "# Example usage:\n",
    "best_params, all_results = explore_parameter_space(features_df, labels)\n",
    "\n",
    "# Use the best parameters found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d2ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Result parameters found: {'r': 0.9, 'merge_threshold': 0.55} \\Result parameters found:        r  merge_threshold  num_clusters  total_samples  avg_cluster_size  \\\n",
      "41  0.60             0.40             3            624        208.000000   \n",
      "26  0.92             0.40             3            624        208.000000   \n",
      "46  0.87             0.45             3            624        208.000000   \n",
      "14  0.90             0.40             3            624        208.000000   \n",
      "13  0.95             0.40             3            624        208.000000   \n",
      "..   ...              ...           ...            ...               ...   \n",
      "29  0.95             0.75             8            624         78.000000   \n",
      "76  0.85             0.80             8            624         78.000000   \n",
      "33  0.90             0.75             8            624         78.000000   \n",
      "51  0.80             0.80            10            624         62.400000   \n",
      "58  0.77             0.80            11            624         56.727273   \n",
      "\n",
      "    median_cluster_size  largest_cluster  smallest_cluster  small_clusters  \\\n",
      "41                    1              622                 1               2   \n",
      "26                    1              622                 1               2   \n",
      "46                    1              622                 1               2   \n",
      "14                    1              622                 1               2   \n",
      "13                    1              622                 1               2   \n",
      "..                  ...              ...               ...             ...   \n",
      "29                   18              496                 1               2   \n",
      "76                   56              316                 1               2   \n",
      "33                   18              496                 1               2   \n",
      "51                   17              293                 1               2   \n",
      "58                   16              211                 1               1   \n",
      "\n",
      "    medium_clusters  large_clusters  elapsed_time  \n",
      "41                0               1     47.397384  \n",
      "26                0               1      1.866479  \n",
      "46                0               1      1.826160  \n",
      "14                0               1      1.802268  \n",
      "13                0               1      1.795085  \n",
      "..              ...             ...           ...  \n",
      "29                0               6      0.836916  \n",
      "76                0               6      0.441516  \n",
      "33                0               6      0.837447  \n",
      "51                1               7      1.404785  \n",
      "58                4               6      4.787552  \n",
      "\n",
      "[83 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\Result parameters found:\", best_params, \"\\Result parameters found:\", all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, densities, similarity = run_pipeline(\n",
    "    features_df, \n",
    "    labels=labels, \n",
    "    k=1000, \n",
    "    r=best_params['r'],\n",
    "    merge_threshold=best_params['merge_threshold'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20afa7e5",
   "metadata": {},
   "source": [
    "# Visualise all best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74550e1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 418\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fig\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    417\u001b[0m fig \u001b[38;5;241m=\u001b[39m visualize_clustering_results(\n\u001b[0;32m--> 418\u001b[0m     clusters\u001b[38;5;241m=\u001b[39m\u001b[43mclusters\u001b[49m, \n\u001b[1;32m    419\u001b[0m     densities\u001b[38;5;241m=\u001b[39mdensities,\n\u001b[1;32m    420\u001b[0m     features_df\u001b[38;5;241m=\u001b[39mfeatures_df, \n\u001b[1;32m    421\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,  \u001b[38;5;66;03m# Optional\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     similarity_matrix\u001b[38;5;241m=\u001b[39msimilarity,  \u001b[38;5;66;03m# Pass the similarity matrix\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     params\u001b[38;5;241m=\u001b[39mbest_params,  \u001b[38;5;66;03m# Pass the parameters used\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     all_results\u001b[38;5;241m=\u001b[39mall_results  \u001b[38;5;66;03m# Pass parameter exploration results\u001b[39;00m\n\u001b[1;32m    425\u001b[0m )\n\u001b[1;32m    426\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclustering_visualization.png\u001b[39m\u001b[38;5;124m'\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, bbox_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    427\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "def visualize_clustering_results(clusters, densities, features_df, labels=None, similarity_matrix=None, params=None, all_results=None):\n",
    "    \"\"\"\n",
    "    Comprehensive visualization of clustering results with multiple plots and analyses.\n",
    "    \n",
    "    Args:\n",
    "        clusters: List of clusters (each cluster is a list of sample indices)\n",
    "        densities: List of cluster sizes\n",
    "        features_df: DataFrame containing the original features\n",
    "        labels: Optional ground truth labels for evaluation\n",
    "        similarity_matrix: Similarity matrix used for clustering\n",
    "        params: Dictionary with the parameters used (r and merge_threshold)\n",
    "        all_results: DataFrame with all parameter exploration results (if available)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "    from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import math\n",
    "    \n",
    "    # Set overall style\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    # Create a color map for clusters\n",
    "    def get_color_map(n_clusters):\n",
    "        # Use distinct colors for up to 20 clusters, then recycle\n",
    "        distinct_colors = plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "        if n_clusters <= 20:\n",
    "            return ListedColormap(distinct_colors[:n_clusters])\n",
    "        else:\n",
    "            # For more clusters, recycle colors\n",
    "            return plt.cm.tab20\n",
    "\n",
    "    # Setup the main figure with multiple subplots\n",
    "    n_clusters = len(clusters)\n",
    "    total_samples = sum(densities)\n",
    "    \n",
    "    print(f\"Visualizing clustering results for {n_clusters} clusters with {total_samples} total samples\")\n",
    "    \n",
    "    # Assign cluster labels to each sample\n",
    "    sample_to_cluster = np.zeros(features_df.shape[0], dtype=int) - 1  # Initialize with -1 (no cluster)\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        for sample_idx in cluster:\n",
    "            sample_to_cluster[sample_idx] = i\n",
    "    \n",
    "    # Setup subplots layout\n",
    "    n_plots = 8 if all_results is not None else 6\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    gs = gridspec.GridSpec(n_plots, 2, height_ratios=[1, 1, 1, 1.5, 1, 1, 1, 1] if n_plots == 8 else [1, 1, 1, 1.5, 1, 1])\n",
    "    \n",
    "    # 1. Cluster Size Distribution\n",
    "    ax1 = plt.subplot(gs[0, 0])\n",
    "    sns.histplot(densities, bins=30, kde=True, ax=ax1)\n",
    "    ax1.set_title(\"Cluster Size Distribution\")\n",
    "    ax1.set_xlabel(\"Cluster Size\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    ax1.axvline(np.mean(densities), color='r', linestyle='--', label=f'Mean: {np.mean(densities):.2f}')\n",
    "    ax1.axvline(np.median(densities), color='g', linestyle='--', label=f'Median: {np.median(densities):.2f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Top Clusters Size Comparison\n",
    "    top_n = min(20, n_clusters)\n",
    "    ax2 = plt.subplot(gs[0, 1])\n",
    "    bars = ax2.bar(range(top_n), densities[:top_n])\n",
    "    ax2.set_title(f\"Size of Top {top_n} Clusters\")\n",
    "    ax2.set_xlabel(\"Cluster Index\")\n",
    "    ax2.set_ylabel(\"Number of Samples\")\n",
    "    \n",
    "    # Add color for bars based on size (darker = larger)\n",
    "    for i, bar in enumerate(bars):\n",
    "        bar.set_color(plt.cm.viridis(densities[i]/max(densities)))\n",
    "    \n",
    "    # 3. Cluster Size Distribution (cumulative)\n",
    "    ax3 = plt.subplot(gs[1, 0])\n",
    "    sorted_sizes = np.sort(densities)[::-1]  # Sort in descending order\n",
    "    cumulative_sizes = np.cumsum(sorted_sizes)\n",
    "    cumulative_percentage = cumulative_sizes / total_samples * 100\n",
    "    \n",
    "    ax3.plot(range(1, len(cumulative_percentage) + 1), cumulative_percentage, 'b-', marker='o', markersize=3)\n",
    "    ax3.set_title(\"Cumulative Sample Distribution\")\n",
    "    ax3.set_xlabel(\"Number of Clusters\")\n",
    "    ax3.set_ylabel(\"Cumulative % of Samples\")\n",
    "    \n",
    "    # Add threshold lines\n",
    "    for threshold in [50, 80, 90, 95]:\n",
    "        # Find the number of clusters needed to reach this threshold\n",
    "        clusters_needed = np.argmax(cumulative_percentage >= threshold) + 1\n",
    "        ax3.axhline(threshold, color='r', linestyle='--', alpha=0.3)\n",
    "        ax3.axvline(clusters_needed, color='g', linestyle='--', alpha=0.3)\n",
    "        ax3.annotate(f\"{threshold}% at {clusters_needed} clusters\", \n",
    "                   xy=(clusters_needed, threshold), \n",
    "                   xytext=(clusters_needed + 5, threshold + 2),\n",
    "                   arrowprops=dict(arrowstyle=\"->\", color=\"black\"))\n",
    "    \n",
    "    # 4. Intra-cluster Similarity Heatmap (for top clusters)\n",
    "    ax4 = plt.subplot(gs[1, 1])\n",
    "    \n",
    "    # Calculate average similarity within and between top clusters\n",
    "    top_k = min(10, n_clusters)  # Show top 10 clusters or fewer\n",
    "    heat_map_size = top_k\n",
    "    \n",
    "    if similarity_matrix is not None:\n",
    "        similarity_heatmap = np.zeros((heat_map_size, heat_map_size))\n",
    "        \n",
    "        # Calculate average similarity within and between clusters\n",
    "        for i in range(heat_map_size):\n",
    "            for j in range(heat_map_size):\n",
    "                cluster_i = clusters[i]\n",
    "                cluster_j = clusters[j]\n",
    "                \n",
    "                # Get all pairwise similarities between samples in the two clusters\n",
    "                similarities = []\n",
    "                for idx_i in cluster_i:\n",
    "                    for idx_j in cluster_j:\n",
    "                        similarities.append(similarity_matrix[idx_i, idx_j])\n",
    "                \n",
    "                # Calculate average similarity\n",
    "                similarity_heatmap[i, j] = np.mean(similarities) if similarities else 0\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(similarity_heatmap, annot=True, cmap=\"YlGnBu\", ax=ax4, \n",
    "                    vmin=0, vmax=1, fmt='.2f', \n",
    "                    xticklabels=range(heat_map_size), \n",
    "                    yticklabels=range(heat_map_size))\n",
    "        ax4.set_title(f\"Similarity Between Top {heat_map_size} Clusters\")\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, \"Similarity matrix not provided\", \n",
    "                horizontalalignment='center', verticalalignment='center')\n",
    "    \n",
    "    # 5. Dimensionality Reduction Visualization (PCA and t-SNE)\n",
    "    # Convert features to numpy for dimensionality reduction\n",
    "    X = features_df.values\n",
    "    \n",
    "    # First subplot: PCA\n",
    "    ax5 = plt.subplot(gs[2, 0])\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Color points by cluster\n",
    "    scatter = ax5.scatter(X_pca[:, 0], X_pca[:, 1], c=sample_to_cluster, \n",
    "                          cmap=get_color_map(n_clusters), \n",
    "                          alpha=0.7, s=10)\n",
    "    ax5.set_title(f\"PCA Visualization of {n_clusters} Clusters\")\n",
    "    \n",
    "    # Add legend for top clusters\n",
    "    top_n_for_legend = min(10, n_clusters)\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                 markerfacecolor=plt.cm.tab20(i % 20), \n",
    "                                 label=f'Cluster {i} (size: {densities[i]})', \n",
    "                                 markersize=8) \n",
    "                      for i in range(top_n_for_legend)]\n",
    "    ax5.legend(handles=legend_elements, title=\"Top Clusters\", \n",
    "               loc=\"upper right\", bbox_to_anchor=(1.7, 1))\n",
    "    \n",
    "    # Second subplot: t-SNE\n",
    "    ax6 = plt.subplot(gs[2, 1])\n",
    "    \n",
    "    # Check if there are enough samples for t-SNE\n",
    "    max_tsne_samples = 5000  # t-SNE can be slow with large datasets\n",
    "    if X.shape[0] > max_tsne_samples:\n",
    "        # Randomly sample a subset\n",
    "        sample_indices = np.random.choice(X.shape[0], max_tsne_samples, replace=False)\n",
    "        X_subset = X[sample_indices]\n",
    "        clusters_subset = sample_to_cluster[sample_indices]\n",
    "        print(f\"Using {max_tsne_samples} random samples for t-SNE visualization\")\n",
    "    else:\n",
    "        X_subset = X\n",
    "        clusters_subset = sample_to_cluster\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    try:\n",
    "        tsne = TSNE(n_components=2, perplexity=min(30, len(X_subset)-1), \n",
    "                   n_iter=300, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(X_subset)\n",
    "        \n",
    "        scatter = ax6.scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters_subset, \n",
    "                              cmap=get_color_map(n_clusters), \n",
    "                              alpha=0.7, s=10)\n",
    "        ax6.set_title(f\"t-SNE Visualization of {n_clusters} Clusters\")\n",
    "    except Exception as e:\n",
    "        ax6.text(0.5, 0.5, f\"t-SNE error: {str(e)}\", \n",
    "                horizontalalignment='center', verticalalignment='center')\n",
    "    \n",
    "    # 6. Cluster Quality Metrics\n",
    "    ax7 = plt.subplot(gs[3, :])\n",
    "    \n",
    "    # Create a DataFrame for cluster quality metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Cluster': range(n_clusters),\n",
    "        'Size': densities,\n",
    "    })\n",
    "    \n",
    "    # Add average intra-cluster similarity\n",
    "    if similarity_matrix is not None:\n",
    "        intra_similarities = []\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            if len(cluster) > 1:\n",
    "                # Get all pairwise similarities within the cluster\n",
    "                cluster_sim = 0\n",
    "                count = 0\n",
    "                for idx_i in range(len(cluster)):\n",
    "                    for idx_j in range(idx_i+1, len(cluster)):\n",
    "                        cluster_sim += similarity_matrix[cluster[idx_i], cluster[idx_j]]\n",
    "                        count += 1\n",
    "                        \n",
    "                avg_sim = cluster_sim / count if count > 0 else 0\n",
    "            else:\n",
    "                avg_sim = 1.0  # Single-element cluster\n",
    "                \n",
    "            intra_similarities.append(avg_sim)\n",
    "            \n",
    "        metrics_df['Intra-Similarity'] = intra_similarities\n",
    "    \n",
    "    # Calculate cluster quality metrics where possible\n",
    "    try:\n",
    "        if len(np.unique(sample_to_cluster[sample_to_cluster >= 0])) > 1 and similarity_matrix is not None:\n",
    "            # Get only samples that are in clusters\n",
    "            clustered_indices = np.where(sample_to_cluster >= 0)[0]\n",
    "            clustered_features = X[clustered_indices]\n",
    "            clustered_labels = sample_to_cluster[clustered_indices]\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            silhouette = silhouette_score(1 - similarity_matrix[clustered_indices][:, clustered_indices], \n",
    "                                          clustered_labels, metric='precomputed')\n",
    "            metrics_df['Silhouette Score'] = silhouette\n",
    "            \n",
    "            # Calculate other metrics if we have enough samples\n",
    "            try:\n",
    "                ch_score = calinski_harabasz_score(clustered_features, clustered_labels)\n",
    "                db_score = davies_bouldin_score(clustered_features, clustered_labels)\n",
    "                metrics_df['CH Score'] = ch_score\n",
    "                metrics_df['DB Score'] = db_score\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating cluster validity metrics: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating silhouette score: {e}\")\n",
    "    \n",
    "    # If ground truth labels are provided, calculate external metrics\n",
    "    if labels is not None:\n",
    "        try:\n",
    "            # Calculate ARI and NMI only for samples assigned to clusters\n",
    "            clustered_indices = np.where(sample_to_cluster >= 0)[0]\n",
    "            true_labels = labels[clustered_indices]\n",
    "            pred_labels = sample_to_cluster[clustered_indices]\n",
    "            \n",
    "            # Calculate external validation metrics\n",
    "            if len(np.unique(true_labels)) > 1:\n",
    "                ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "                nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "                metrics_df['ARI'] = ari\n",
    "                metrics_df['NMI'] = nmi\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating external validation metrics: {e}\")\n",
    "    \n",
    "    # Plot metrics as a horizontal bar chart\n",
    "    metric_columns = [col for col in metrics_df.columns if col != 'Cluster' and col != 'Size']\n",
    "    \n",
    "    if metric_columns:\n",
    "        # Fixed the error with metrics table: Convert explicitly to strings for table cells\n",
    "        metrics_mean = metrics_df[metric_columns].mean().to_frame().T\n",
    "        metrics_mean['Metric'] = 'Average'\n",
    "        \n",
    "        # Convert the metrics to string with formatting applied\n",
    "        cell_text = []\n",
    "        for _, row in metrics_mean.iterrows():\n",
    "            row_text = ['Average']  # Start with the row label\n",
    "            for col in metric_columns:\n",
    "                # Format numeric values to 3 decimal places\n",
    "                if isinstance(row[col], (int, float)):\n",
    "                    row_text.append(f\"{row[col]:.3f}\")\n",
    "                else:\n",
    "                    row_text.append(str(row[col]))\n",
    "            cell_text.append(row_text)\n",
    "        \n",
    "        metrics_table = ax7.table(\n",
    "            cellText=cell_text,\n",
    "            colLabels=['Metric'] + metric_columns,\n",
    "            loc='center',\n",
    "            cellLoc='center'\n",
    "        )\n",
    "        metrics_table.auto_set_font_size(False)\n",
    "        metrics_table.set_fontsize(12)\n",
    "        metrics_table.scale(1, 2)\n",
    "        ax7.axis('off')\n",
    "        ax7.set_title(\"Clustering Quality Metrics\", pad=20)\n",
    "    else:\n",
    "        ax7.text(0.5, 0.5, \"No metrics calculated\", \n",
    "                horizontalalignment='center', verticalalignment='center')\n",
    "    \n",
    "    # 7. Parameter Investigation (if parameter exploration results are available)\n",
    "    if all_results is not None:\n",
    "        # Plot parameter heatmap\n",
    "        ax8 = plt.subplot(gs[4, :])\n",
    "        \n",
    "        # Create pivot table for parameter exploration\n",
    "        try:\n",
    "            pivot = all_results.pivot_table(\n",
    "                index='r', \n",
    "                columns='merge_threshold', \n",
    "                values='num_clusters',\n",
    "                aggfunc='first'\n",
    "            )\n",
    "            \n",
    "            # Plot heatmap\n",
    "            sns.heatmap(pivot, annot=True, cmap=\"YlGnBu\", ax=ax8, fmt='g')\n",
    "            ax8.set_title(\"Number of Clusters by Parameter Combination\")\n",
    "            \n",
    "            # Highlight the current parameters if provided\n",
    "            if params and 'r' in params and 'merge_threshold' in params:\n",
    "                r_idx = np.where(pivot.index == params['r'])[0]\n",
    "                m_idx = np.where(pivot.columns == params['merge_threshold'])[0]\n",
    "                \n",
    "                if len(r_idx) > 0 and len(m_idx) > 0:\n",
    "                    r_pos = r_idx[0]\n",
    "                    m_pos = m_idx[0]\n",
    "                    ax8.add_patch(plt.Rectangle((m_pos, r_pos), 1, 1, fill=False, \n",
    "                                             edgecolor='red', lw=3))\n",
    "        except Exception as e:\n",
    "            ax8.text(0.5, 0.5, f\"Error creating parameter heatmap: {str(e)}\", \n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "        \n",
    "        # Plot parameter impact on cluster count\n",
    "        ax9 = plt.subplot(gs[5, 0])\n",
    "        ax10 = plt.subplot(gs[5, 1])\n",
    "        \n",
    "        # Effect of r on number of clusters\n",
    "        r_impact = all_results.groupby('r')['num_clusters'].mean().reset_index()\n",
    "        sns.lineplot(data=r_impact, x='r', y='num_clusters', ax=ax9, marker='o')\n",
    "        ax9.set_title(\"Effect of r on Cluster Count\")\n",
    "        ax9.set_xlabel(\"r (similarity threshold)\")\n",
    "        ax9.set_ylabel(\"Average Number of Clusters\")\n",
    "        \n",
    "        # Effect of merge_threshold on number of clusters\n",
    "        m_impact = all_results.groupby('merge_threshold')['num_clusters'].mean().reset_index()\n",
    "        sns.lineplot(data=m_impact, x='merge_threshold', y='num_clusters', ax=ax10, marker='o')\n",
    "        ax10.set_title(\"Effect of merge_threshold on Cluster Count\")\n",
    "        ax10.set_xlabel(\"merge_threshold\")\n",
    "        ax10.set_ylabel(\"Average Number of Clusters\")\n",
    "        \n",
    "        # Additional visualizations for parameter exploration\n",
    "        ax11 = plt.subplot(gs[6, 0])\n",
    "        ax12 = plt.subplot(gs[6, 1])\n",
    "        \n",
    "        # Scatterplot of parameters colored by cluster count\n",
    "        scatter = ax11.scatter(all_results['r'], all_results['merge_threshold'], \n",
    "                            c=all_results['num_clusters'], cmap='viridis', \n",
    "                            s=100, alpha=0.7)\n",
    "        if params and 'r' in params and 'merge_threshold' in params:\n",
    "            ax11.scatter([params['r']], [params['merge_threshold']], \n",
    "                       color='red', s=200, marker='*', \n",
    "                       label=f\"Current (r={params['r']}, m={params['merge_threshold']})\")\n",
    "            ax11.legend()\n",
    "            \n",
    "        plt.colorbar(scatter, ax=ax11, label=\"Number of Clusters\")\n",
    "        ax11.set_title(\"Parameter Space Exploration\")\n",
    "        ax11.set_xlabel(\"r (similarity threshold)\")\n",
    "        ax11.set_ylabel(\"merge_threshold\")\n",
    "        \n",
    "        # Plot distribution of cluster counts\n",
    "        sns.histplot(all_results['num_clusters'], kde=True, ax=ax12)\n",
    "        ax12.set_title(\"Distribution of Cluster Counts\")\n",
    "        ax12.set_xlabel(\"Number of Clusters\")\n",
    "        ax12.set_ylabel(\"Frequency\")\n",
    "        \n",
    "        if params and 'r' in params and 'merge_threshold' in params:\n",
    "            current_clusters = n_clusters\n",
    "            ax12.axvline(current_clusters, color='r', linestyle='--', \n",
    "                       label=f\"Current: {current_clusters}\")\n",
    "            ax12.legend()\n",
    "        \n",
    "        # Final parameter summary - fixed table formatting\n",
    "        ax13 = plt.subplot(gs[7, :])\n",
    "        \n",
    "        # Create a summary table of best parameter combinations\n",
    "        best_params = all_results.sort_values('num_clusters').head(10)\n",
    "        best_params = best_params[['r', 'merge_threshold', 'num_clusters', \n",
    "                                 'avg_cluster_size', 'largest_cluster']]\n",
    "        \n",
    "        # Format the table with strings\n",
    "        cell_text = []\n",
    "        for _, row in best_params.iterrows():\n",
    "            cell_text.append([\n",
    "                f\"{row['r']:.2f}\",\n",
    "                f\"{row['merge_threshold']:.2f}\",\n",
    "                f\"{int(row['num_clusters'])}\",\n",
    "                f\"{row['avg_cluster_size']:.1f}\",\n",
    "                f\"{int(row['largest_cluster'])}\"\n",
    "            ])\n",
    "            \n",
    "        param_table = ax13.table(\n",
    "            cellText=cell_text,\n",
    "            colLabels=['r', 'merge_threshold', 'num_clusters', 'avg_size', 'largest_cluster'],\n",
    "            loc='center',\n",
    "            cellLoc='center'\n",
    "        )\n",
    "        param_table.auto_set_font_size(False)\n",
    "        param_table.set_fontsize(12)\n",
    "        param_table.scale(1, 2)\n",
    "        ax13.axis('off')\n",
    "        ax13.set_title(\"Top 10 Parameter Combinations with Fewest Clusters\", pad=20)\n",
    "    \n",
    "    # Make layout tight and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "fig = visualize_clustering_results(\n",
    "    clusters=clusters, \n",
    "    densities=densities,\n",
    "    features_df=features_df, \n",
    "    labels=labels,  # Optional\n",
    "    similarity_matrix=similarity,  # Pass the similarity matrix\n",
    "    params=best_params,  # Pass the parameters used\n",
    "    all_results=all_results  # Pass parameter exploration results\n",
    ")\n",
    "plt.savefig('clustering_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e410985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory: cluster_analysis\n",
      "Visualizing clustering results for 11 clusters with 624 total samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/5k5grmpn7m1683gh7vt2xhsw0000gn/T/ipykernel_6036/2730747601.py:162: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"b-\" (-> color='b'). The keyword argument will take precedence.\n",
      "  ax.plot(\n",
      "/var/folders/px/5k5grmpn7m1683gh7vt2xhsw0000gn/T/ipykernel_6036/2730747601.py:294: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colors = get_cmap('tab20')\n",
      "/Users/ankitaniket/Downloads/Final Year Project/clustering-algo-code/.venv/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating external validation metrics: only integer scalar arrays can be converted to a scalar index\n",
      "HTML report generated: cluster_analysis/clustering_report.html\n",
      "All visualizations saved to cluster_analysis/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_clustering_detailed(clusters, densities, features_df, labels=None, similarity_matrix=None, params=None, all_results=None, output_dir=\"cluster_visualizations\"):\n",
    "    \"\"\"\n",
    "    Creates separate high-quality visualizations for each aspect of clustering results\n",
    "    \n",
    "    Args:\n",
    "        clusters: List of clusters (each cluster is a list of sample indices)\n",
    "        densities: List of cluster sizes\n",
    "        features_df: DataFrame containing the original features\n",
    "        labels: Optional ground truth labels for evaluation\n",
    "        similarity_matrix: Similarity matrix used for clustering\n",
    "        params: Dictionary with the parameters used (r and merge_threshold)\n",
    "        all_results: DataFrame with all parameter exploration results (if available)\n",
    "        output_dir: Directory to save visualizations (will be created if it doesn't exist)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "    from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import os\n",
    "    from matplotlib.cm import get_cmap\n",
    "    import matplotlib.ticker as ticker\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "    \n",
    "    # Set overall style\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    n_clusters = len(clusters)\n",
    "    total_samples = sum(densities)\n",
    "    print(f\"Visualizing clustering results for {n_clusters} clusters with {total_samples} total samples\")\n",
    "    \n",
    "    # Assign cluster labels to each sample\n",
    "    sample_to_cluster = np.zeros(features_df.shape[0], dtype=int) - 1  # Initialize with -1 (no cluster)\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        for sample_idx in cluster:\n",
    "            sample_to_cluster[sample_idx] = i\n",
    "    \n",
    "    # Helper function to create color map\n",
    "    def get_color_map(n_clusters):\n",
    "        # Use distinct colors for up to 20 clusters, then recycle\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "        return colors[:min(n_clusters, 20)]\n",
    "    \n",
    "    # 1. Cluster Size Distribution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Create custom bins to handle different distributions\n",
    "    max_cluster_size = max(densities)\n",
    "    if max_cluster_size > 100:\n",
    "        bins = np.linspace(0, max_cluster_size, 30)\n",
    "    else:\n",
    "        bins = 20\n",
    "        \n",
    "    sns.histplot(densities, bins=bins, kde=True, ax=ax, color='royalblue', alpha=0.7)\n",
    "    ax.set_title(\"Cluster Size Distribution\", fontsize=16)\n",
    "    ax.set_xlabel(\"Cluster Size\", fontsize=14)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=14)\n",
    "    \n",
    "    # Add mean and median lines\n",
    "    mean_size = np.mean(densities)\n",
    "    median_size = np.median(densities)\n",
    "    ax.axvline(mean_size, color='darkred', linestyle='--', linewidth=2, \n",
    "               label=f'Mean: {mean_size:.2f}')\n",
    "    ax.axvline(median_size, color='darkgreen', linestyle='--', linewidth=2, \n",
    "               label=f'Median: {median_size:.2f}')\n",
    "    \n",
    "    ax.legend(fontsize=12, loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/1_cluster_size_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Top Clusters Size Comparison\n",
    "    # Create a custom chart showing the top 5 clusters with details\n",
    "    top_n = 5  # Show top 5 clusters\n",
    "    \n",
    "    # Create a DataFrame with cluster information\n",
    "    top_clusters_df = pd.DataFrame({\n",
    "        'Cluster': range(n_clusters),\n",
    "        'Size': densities\n",
    "    }).sort_values('Size', ascending=False).head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Use a specific colormap\n",
    "    colors = plt.cm.viridis(np.linspace(0.1, 0.9, top_n))\n",
    "    \n",
    "    bars = ax.bar(\n",
    "        top_clusters_df['Cluster'], \n",
    "        top_clusters_df['Size'],\n",
    "        color=colors,\n",
    "        width=0.6\n",
    "    )\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height + 5,\n",
    "            f'{int(height)}',\n",
    "            ha='center', \n",
    "            va='bottom',\n",
    "            fontsize=12,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    ax.set_title(f\"Top {top_n} Largest Clusters\", fontsize=16)\n",
    "    ax.set_xlabel(\"Cluster ID\", fontsize=14)\n",
    "    ax.set_ylabel(\"Number of Samples\", fontsize=14)\n",
    "    \n",
    "    # Add percentage of total samples for each top cluster\n",
    "    for i, (_, row) in enumerate(top_clusters_df.iterrows()):\n",
    "        percent = (row['Size'] / total_samples) * 100\n",
    "        ax.annotate(\n",
    "            f\"{percent:.1f}% of total\",\n",
    "            xy=(row['Cluster'], row['Size'] / 2),\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            fontsize=11,\n",
    "            color='white',\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    # Add total percentage covered by top N clusters\n",
    "    top_n_percent = (top_clusters_df['Size'].sum() / total_samples) * 100\n",
    "    ax.text(\n",
    "        0.5, 0.01,\n",
    "        f\"These {top_n} clusters contain {top_n_percent:.1f}% of all samples\",\n",
    "        transform=ax.transAxes,\n",
    "        ha='center',\n",
    "        fontsize=13,\n",
    "        fontweight='bold',\n",
    "        bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.5')\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/2_top_{top_n}_clusters.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Cumulative Sample Distribution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    sorted_sizes = np.sort(densities)[::-1]  # Sort in descending order\n",
    "    cumulative_sizes = np.cumsum(sorted_sizes)\n",
    "    cumulative_percentage = cumulative_sizes / total_samples * 100\n",
    "    \n",
    "    # Plot cumulative distribution\n",
    "    ax.plot(\n",
    "        range(1, len(cumulative_percentage) + 1), \n",
    "        cumulative_percentage, \n",
    "        'b-', \n",
    "        marker='o', \n",
    "        markersize=5,\n",
    "        linewidth=2,\n",
    "        color='royalblue'\n",
    "    )\n",
    "    \n",
    "    ax.set_title(\"Cumulative Sample Distribution\", fontsize=16)\n",
    "    ax.set_xlabel(\"Number of Clusters\", fontsize=14)\n",
    "    ax.set_ylabel(\"Cumulative % of Samples\", fontsize=14)\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add threshold lines with better visibility\n",
    "    for threshold in [50, 80, 90, 95]:\n",
    "        # Find the number of clusters needed to reach this threshold\n",
    "        clusters_needed = np.argmax(cumulative_percentage >= threshold) + 1\n",
    "        \n",
    "        # Only add if we have enough clusters to reach this threshold\n",
    "        if clusters_needed <= len(cumulative_percentage):\n",
    "            # Add horizontal line\n",
    "            ax.axhline(\n",
    "                threshold, \n",
    "                color='crimson', \n",
    "                linestyle='--', \n",
    "                alpha=0.5,\n",
    "                linewidth=1.5\n",
    "            )\n",
    "            \n",
    "            # Add vertical line\n",
    "            ax.axvline(\n",
    "                clusters_needed, \n",
    "                color='darkgreen', \n",
    "                linestyle='--', \n",
    "                alpha=0.5,\n",
    "                linewidth=1.5\n",
    "            )\n",
    "            \n",
    "            # Add annotation with arrow\n",
    "            ax.annotate(\n",
    "                f\"{threshold}% at {clusters_needed} clusters\", \n",
    "                xy=(clusters_needed, threshold), \n",
    "                xytext=(clusters_needed + max(1, n_clusters/20), threshold + 5),\n",
    "                arrowprops=dict(\n",
    "                    arrowstyle=\"->\", \n",
    "                    color=\"black\",\n",
    "                    linewidth=1.5\n",
    "                ),\n",
    "                fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n",
    "            )\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 105)\n",
    "    \n",
    "    # Set x-axis limit to show a bit more than we need\n",
    "    ax.set_xlim(0, min(n_clusters * 1.1, n_clusters + 5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/3_cumulative_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Intra-cluster Similarity Heatmap (for top clusters)\n",
    "    if similarity_matrix is not None:\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # Calculate average similarity within and between top clusters\n",
    "        top_k = min(10, n_clusters)  # Show top 10 clusters or fewer\n",
    "        top_clusters = np.argsort(densities)[::-1][:top_k]\n",
    "        \n",
    "        similarity_heatmap = np.zeros((top_k, top_k))\n",
    "        \n",
    "        # Calculate average similarity within and between clusters\n",
    "        for i, cluster_i_idx in enumerate(top_clusters):\n",
    "            for j, cluster_j_idx in enumerate(top_clusters):\n",
    "                cluster_i = clusters[cluster_i_idx]\n",
    "                cluster_j = clusters[cluster_j_idx]\n",
    "                \n",
    "                # Get all pairwise similarities between samples in the two clusters\n",
    "                similarities = []\n",
    "                for idx_i in cluster_i:\n",
    "                    for idx_j in cluster_j:\n",
    "                        similarities.append(similarity_matrix[idx_i, idx_j])\n",
    "                \n",
    "                # Calculate average similarity\n",
    "                similarity_heatmap[i, j] = np.mean(similarities) if similarities else 0\n",
    "        \n",
    "        # Create custom labels showing cluster index and size\n",
    "        labels = [f\"{idx}\\n(size: {densities[idx]})\" for idx in top_clusters]\n",
    "        \n",
    "        # Plot heatmap with improved aesthetics\n",
    "        sns.heatmap(\n",
    "            similarity_heatmap, \n",
    "            annot=True, \n",
    "            cmap=\"YlGnBu\", \n",
    "            ax=ax, \n",
    "            vmin=0, \n",
    "            vmax=1, \n",
    "            fmt='.2f', \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels,\n",
    "            linewidths=0.5,\n",
    "            annot_kws={\"size\": 10, \"weight\": \"bold\"}\n",
    "        )\n",
    "        \n",
    "        # Add titles and labels\n",
    "        ax.set_title(f\"Similarity Between Top {top_k} Clusters\", fontsize=16)\n",
    "        ax.set_xlabel(\"Cluster ID (size)\", fontsize=14)\n",
    "        ax.set_ylabel(\"Cluster ID (size)\", fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/4_similarity_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 5. Dimensionality Reduction Visualization (PCA)\n",
    "    # Convert features to numpy for dimensionality reduction\n",
    "    X = features_df.values\n",
    "    \n",
    "    # Apply PCA\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Get a colormap\n",
    "    colors = get_cmap('tab20')\n",
    "    \n",
    "    # Plot the top 10 largest clusters with different colors\n",
    "    top_clusters = np.argsort(densities)[::-1][:10]\n",
    "    \n",
    "    legend_elements = []\n",
    "    \n",
    "    # Plot unclustered points first (if any)\n",
    "    unclustered = np.where(sample_to_cluster == -1)[0]\n",
    "    if len(unclustered) > 0:\n",
    "        ax.scatter(\n",
    "            X_pca[unclustered, 0], \n",
    "            X_pca[unclustered, 1], \n",
    "            c='lightgray', \n",
    "            alpha=0.5, \n",
    "            s=20,\n",
    "            label=\"Unclustered\"\n",
    "        )\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                      markerfacecolor='lightgray', \n",
    "                                      label=f'Unclustered ({len(unclustered)})', \n",
    "                                      markersize=10))\n",
    "    \n",
    "    # Plot each top cluster\n",
    "    for i, cluster_idx in enumerate(top_clusters):\n",
    "        cluster_samples = np.where(sample_to_cluster == cluster_idx)[0]\n",
    "        ax.scatter(\n",
    "            X_pca[cluster_samples, 0],\n",
    "            X_pca[cluster_samples, 1],\n",
    "            c=[colors(i % 20)],\n",
    "            alpha=0.7,\n",
    "            s=30,\n",
    "            label=f\"Cluster {cluster_idx} (size: {densities[cluster_idx]})\"\n",
    "        )\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                      markerfacecolor=colors(i % 20), \n",
    "                                      label=f'Cluster {cluster_idx} (size: {densities[cluster_idx]})', \n",
    "                                      markersize=10))\n",
    "    \n",
    "    # Add variance explained\n",
    "    var_explained = pca.explained_variance_ratio_\n",
    "    ax.set_xlabel(f\"PC1 ({var_explained[0]:.2%} variance)\", fontsize=14)\n",
    "    ax.set_ylabel(f\"PC2 ({var_explained[1]:.2%} variance)\", fontsize=14)\n",
    "    \n",
    "    ax.set_title(f\"PCA Visualization of Top Clusters\", fontsize=16)\n",
    "    \n",
    "    # Add legend with better positioning\n",
    "    ax.legend(\n",
    "        handles=legend_elements, \n",
    "        title=\"Clusters\", \n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(1.15, 1),\n",
    "        fontsize=12\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/5_pca_visualization.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. t-SNE Visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Check if there are enough samples for t-SNE\n",
    "    max_tsne_samples = 5000  # t-SNE can be slow with large datasets\n",
    "    if X.shape[0] > max_tsne_samples:\n",
    "        # Randomly sample a subset\n",
    "        np.random.seed(42)  # for reproducibility\n",
    "        sample_indices = np.random.choice(X.shape[0], max_tsne_samples, replace=False)\n",
    "        X_subset = X[sample_indices]\n",
    "        clusters_subset = sample_to_cluster[sample_indices]\n",
    "        print(f\"Using {max_tsne_samples} random samples for t-SNE visualization\")\n",
    "    else:\n",
    "        X_subset = X\n",
    "        clusters_subset = sample_to_cluster\n",
    "    \n",
    "    # Apply t-SNE with error handling\n",
    "    try:\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=min(30, len(X_subset)-1), \n",
    "            n_iter=1000,  # Increased for better convergence\n",
    "            random_state=42\n",
    "        )\n",
    "        X_tsne = tsne.fit_transform(X_subset)\n",
    "        \n",
    "        # Plot unclustered points first (if any)\n",
    "        unclustered = np.where(clusters_subset == -1)[0]\n",
    "        if len(unclustered) > 0:\n",
    "            ax.scatter(\n",
    "                X_tsne[unclustered, 0], \n",
    "                X_tsne[unclustered, 1], \n",
    "                c='lightgray', \n",
    "                alpha=0.5, \n",
    "                s=20,\n",
    "                label=\"Unclustered\"\n",
    "            )\n",
    "            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                          markerfacecolor='lightgray', \n",
    "                                          label=f'Unclustered', \n",
    "                                          markersize=10))\n",
    "        \n",
    "        # Plot each top cluster\n",
    "        legend_elements = []\n",
    "        for i, cluster_idx in enumerate(top_clusters):\n",
    "            cluster_samples = np.where(clusters_subset == cluster_idx)[0]\n",
    "            if len(cluster_samples) > 0:\n",
    "                ax.scatter(\n",
    "                    X_tsne[cluster_samples, 0],\n",
    "                    X_tsne[cluster_samples, 1],\n",
    "                    c=[colors(i % 20)],\n",
    "                    alpha=0.7,\n",
    "                    s=30,\n",
    "                    label=f\"Cluster {cluster_idx} (size: {densities[cluster_idx]})\"\n",
    "                )\n",
    "                legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                              markerfacecolor=colors(i % 20), \n",
    "                                              label=f'Cluster {cluster_idx}', \n",
    "                                              markersize=10))\n",
    "        \n",
    "        ax.set_title(f\"t-SNE Visualization of Top Clusters\", fontsize=16)\n",
    "        \n",
    "        # Add legend with better positioning\n",
    "        ax.legend(\n",
    "            handles=legend_elements, \n",
    "            title=\"Clusters\", \n",
    "            loc=\"upper right\",\n",
    "            bbox_to_anchor=(1.15, 1),\n",
    "            fontsize=12\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        ax.text(0.5, 0.5, f\"t-SNE error: {str(e)}\", \n",
    "               horizontalalignment='center', verticalalignment='center',\n",
    "               fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/6_tsne_visualization.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 7. Cluster Quality Metrics Table\n",
    "    # Create a DataFrame for cluster quality metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Cluster': range(n_clusters),\n",
    "        'Size': densities,\n",
    "    })\n",
    "    \n",
    "    # Add average intra-cluster similarity\n",
    "    if similarity_matrix is not None:\n",
    "        intra_similarities = []\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            if len(cluster) > 1:\n",
    "                # Get all pairwise similarities within the cluster\n",
    "                cluster_sim = 0\n",
    "                count = 0\n",
    "                for idx_i in range(len(cluster)):\n",
    "                    for idx_j in range(idx_i+1, len(cluster)):\n",
    "                        cluster_sim += similarity_matrix[cluster[idx_i], cluster[idx_j]]\n",
    "                        count += 1\n",
    "                        \n",
    "                avg_sim = cluster_sim / count if count > 0 else 0\n",
    "            else:\n",
    "                avg_sim = 1.0  # Single-element cluster\n",
    "                \n",
    "            intra_similarities.append(avg_sim)\n",
    "            \n",
    "        metrics_df['Intra-Similarity'] = intra_similarities\n",
    "    \n",
    "    # Calculate cluster quality metrics where possible\n",
    "    metrics_values = {}\n",
    "    \n",
    "    try:\n",
    "        if len(np.unique(sample_to_cluster[sample_to_cluster >= 0])) > 1 and similarity_matrix is not None:\n",
    "            # Get only samples that are in clusters\n",
    "            clustered_indices = np.where(sample_to_cluster >= 0)[0]\n",
    "            clustered_features = X[clustered_indices]\n",
    "            clustered_labels = sample_to_cluster[clustered_indices]\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            silhouette = silhouette_score(1 - similarity_matrix[clustered_indices][:, clustered_indices], \n",
    "                                         clustered_labels, metric='precomputed')\n",
    "            metrics_values['Silhouette Score'] = silhouette\n",
    "            \n",
    "            # Calculate other metrics if we have enough samples\n",
    "            try:\n",
    "                ch_score = calinski_harabasz_score(clustered_features, clustered_labels)\n",
    "                db_score = davies_bouldin_score(clustered_features, clustered_labels)\n",
    "                metrics_values['CH Score'] = ch_score\n",
    "                metrics_values['DB Score'] = db_score\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating cluster validity metrics: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating silhouette score: {e}\")\n",
    "    \n",
    "    # If ground truth labels are provided, calculate external metrics\n",
    "    if labels is not None:\n",
    "        try:\n",
    "            # Calculate ARI and NMI only for samples assigned to clusters\n",
    "            clustered_indices = np.where(sample_to_cluster >= 0)[0]\n",
    "            true_labels = labels[clustered_indices]\n",
    "            pred_labels = sample_to_cluster[clustered_indices]\n",
    "            \n",
    "            # Calculate external validation metrics\n",
    "            if len(np.unique(true_labels)) > 1:\n",
    "                ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "                nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "                metrics_values['ARI'] = ari\n",
    "                metrics_values['NMI'] = nmi\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating external validation metrics: {e}\")\n",
    "    \n",
    "    # Create a detailed metrics visualization\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    ax = plt.gca()\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Prepare data for the table\n",
    "    if metrics_values:\n",
    "        # Format the metrics values\n",
    "        formatted_values = []\n",
    "        for key, value in metrics_values.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                formatted_values.append(f\"{value:.4f}\")\n",
    "            else:\n",
    "                formatted_values.append(str(value))\n",
    "        \n",
    "        metrics_table = ax.table(\n",
    "            cellText=[formatted_values],\n",
    "            colLabels=list(metrics_values.keys()),\n",
    "            loc='center',\n",
    "            cellLoc='center'\n",
    "        )\n",
    "        \n",
    "        # Style the table\n",
    "        metrics_table.auto_set_font_size(False)\n",
    "        metrics_table.set_fontsize(12)\n",
    "        metrics_table.scale(1, 2)\n",
    "        \n",
    "        # Add title\n",
    "        plt.suptitle(\"Clustering Quality Metrics\", fontsize=16, y=0.8)\n",
    "        \n",
    "        # Add description of metrics\n",
    "        descriptions = {\n",
    "            'Silhouette Score': \"Measures how similar a point is to its own cluster compared to others. Range: [-1, 1], higher is better.\",\n",
    "            'CH Score': \"Calinski-Harabasz index - ratio of between-cluster to within-cluster dispersion. Higher is better.\",\n",
    "            'DB Score': \"Davies-Bouldin index - average similarity of each cluster with its most similar cluster. Lower is better.\",\n",
    "            'ARI': \"Adjusted Rand Index - similarity between true and predicted labels, adjusted for chance. Range: [-1, 1], higher is better.\",\n",
    "            'NMI': \"Normalized Mutual Information - normalized measure of dependence between true and predicted labels. Range: [0, 1], higher is better.\"\n",
    "        }\n",
    "        \n",
    "        desc_text = \"\"\n",
    "        for key in metrics_values.keys():\n",
    "            if key in descriptions:\n",
    "                desc_text += f\"• {key}: {descriptions[key]}\\n\"\n",
    "        \n",
    "        plt.figtext(0.5, 0.3, desc_text, ha='center', fontsize=10, \n",
    "                   bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"No metrics calculated\", \n",
    "                ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/7_cluster_quality_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 8. Top 5 Clusters Detailed Analysis\n",
    "    # For a detailed look at the top 5 clusters\n",
    "    top_5_clusters = np.argsort(densities)[::-1][:5]\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create a subplot for each top cluster's detailed analysis\n",
    "    for i, cluster_idx in enumerate(top_5_clusters):\n",
    "        ax = plt.subplot(3, 2, i+1)\n",
    "        \n",
    "        # Get the cluster members\n",
    "        cluster = clusters[cluster_idx]\n",
    "        cluster_size = len(cluster)\n",
    "        \n",
    "        # Calculate intra-cluster statistics\n",
    "        if similarity_matrix is not None:\n",
    "            intra_similarities = []\n",
    "            for idx_i in range(len(cluster)):\n",
    "                for idx_j in range(idx_i+1, len(cluster)):\n",
    "                    intra_similarities.append(similarity_matrix[cluster[idx_i], cluster[idx_j]])\n",
    "            \n",
    "            mean_sim = np.mean(intra_similarities) if intra_similarities else 0\n",
    "            min_sim = np.min(intra_similarities) if intra_similarities else 0\n",
    "            max_sim = np.max(intra_similarities) if intra_similarities else 0\n",
    "            \n",
    "            # Create a histogram of intra-cluster similarities\n",
    "            if intra_similarities:\n",
    "                sns.histplot(intra_similarities, kde=True, ax=ax, color=colors(i % 20), alpha=0.7)\n",
    "                \n",
    "                # Add vertical lines for mean, min, max\n",
    "                ax.axvline(mean_sim, color='red', linestyle='--', \n",
    "                          label=f'Mean: {mean_sim:.3f}')\n",
    "                ax.axvline(min_sim, color='blue', linestyle=':', \n",
    "                          label=f'Min: {min_sim:.3f}')\n",
    "                ax.axvline(max_sim, color='green', linestyle=':', \n",
    "                          label=f'Max: {max_sim:.3f}')\n",
    "                \n",
    "                ax.set_title(f\"Cluster {cluster_idx} (Size: {cluster_size})\", fontsize=14)\n",
    "                ax.set_xlabel(\"Intra-cluster Similarity\", fontsize=12)\n",
    "                ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "                ax.legend(fontsize=10)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Similarity matrix not provided\", \n",
    "                   ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Detailed Analysis of Top 5 Clusters\", fontsize=16, y=1.02)\n",
    "    plt.savefig(f\"{output_dir}/8_top_5_clusters_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 9. Parameter Space Visualization (if parameter exploration results are available)\n",
    "    if all_results is not None:\n",
    "        # 9.1 Parameter Heatmap\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # Create pivot table for parameter exploration\n",
    "        try:\n",
    "            pivot = all_results.pivot_table(\n",
    "                index='r', \n",
    "                columns='merge_threshold', \n",
    "                values='num_clusters',\n",
    "                aggfunc='first'\n",
    "            )\n",
    "            \n",
    "            # Sort indices for better visualization\n",
    "            pivot = pivot.sort_index(ascending=False)\n",
    "            pivot = pivot.sort_index(axis=1)\n",
    "            \n",
    "            # Plot heatmap\n",
    "            sns.heatmap(\n",
    "                pivot, \n",
    "                annot=True, \n",
    "                cmap=\"YlGnBu\", \n",
    "                ax=ax, \n",
    "                fmt='g',\n",
    "                linewidths=0.5,\n",
    "                annot_kws={\"size\": 10, \"weight\": \"bold\"}\n",
    "            )\n",
    "            \n",
    "            ax.set_title(\"Number of Clusters by Parameter Combination\", fontsize=16)\n",
    "            ax.set_xlabel(\"merge_threshold\", fontsize=14)\n",
    "            ax.set_ylabel(\"r (similarity threshold)\", fontsize=14)\n",
    "            \n",
    "            # Highlight the current parameters if provided\n",
    "            if params and 'r' in params and 'merge_threshold' in params:\n",
    "                try:\n",
    "                    r_idx = np.where(pivot.index == params['r'])[0]\n",
    "                    m_idx = np.where(pivot.columns == params['merge_threshold'])[0]\n",
    "                    \n",
    "                    if len(r_idx) > 0 and len(m_idx) > 0:\n",
    "                        r_pos = r_idx[0]\n",
    "                        m_pos = m_idx[0]\n",
    "                        ax.add_patch(plt.Rectangle((m_pos, r_pos), 1, 1, fill=False, \n",
    "                                                 edgecolor='red', lw=3))\n",
    "                        \n",
    "                        # Add text annotation\n",
    "                        plt.text(\n",
    "                            0.5, 0.01,\n",
    "                            f\"Current parameters: r={params['r']}, merge_threshold={params['merge_threshold']}\",\n",
    "                            transform=ax.transAxes,\n",
    "                            ha='center',\n",
    "                            fontsize=12,\n",
    "                            fontweight='bold',\n",
    "                            bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.5')\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error highlighting current parameters: {e}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f\"Error creating parameter heatmap: {str(e)}\", \n",
    "                   ha='center', va='center', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/9_parameter_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 9.2 Parameter Impact on Cluster Count\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        \n",
    "        # Effect of r on number of clusters\n",
    "        r_impact = all_results.groupby('r')['num_clusters'].mean().reset_index()\n",
    "        r_impact = r_impact.sort_values('r')\n",
    "        \n",
    "        sns.lineplot(data=r_impact, x='r', y='num_clusters', ax=ax1, \n",
    "                    marker='o', linewidth=2, markersize=8, color='royalblue')\n",
    "        ax1.set_title(\"Effect of r on Cluster Count\", fontsize=16)\n",
    "        ax1.set_xlabel(\"r (similarity threshold)\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Average Number of Clusters\", fontsize=14)\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add current r parameter if available\n",
    "        if params and 'r' in params:\n",
    "            ax1.axvline(params['r'], color='red', linestyle='--', \n",
    "                      label=f\"Current r={params['r']}\")\n",
    "            ax1.legend(fontsize=12)\n",
    "\n",
    "# Effect of merge_threshold on number of clusters\n",
    "        m_impact = all_results.groupby('merge_threshold')['num_clusters'].mean().reset_index()\n",
    "        m_impact = m_impact.sort_values('merge_threshold')\n",
    "        \n",
    "        sns.lineplot(data=m_impact, x='merge_threshold', y='num_clusters', ax=ax2, \n",
    "                    marker='o', linewidth=2, markersize=8, color='forestgreen')\n",
    "        ax2.set_title(\"Effect of merge_threshold on Cluster Count\", fontsize=16)\n",
    "        ax2.set_xlabel(\"merge_threshold\", fontsize=14)\n",
    "        ax2.set_ylabel(\"Average Number of Clusters\", fontsize=14)\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add current merge_threshold parameter if available\n",
    "        if params and 'merge_threshold' in params:\n",
    "            ax2.axvline(params['merge_threshold'], color='red', linestyle='--', \n",
    "                      label=f\"Current m={params['merge_threshold']}\")\n",
    "            ax2.legend(fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/10_parameter_impact.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 9.3 Parameter Space Exploration\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # Scatterplot of parameters colored by cluster count\n",
    "        scatter = ax.scatter(all_results['r'], all_results['merge_threshold'], \n",
    "                           c=all_results['num_clusters'], cmap='viridis', \n",
    "                           s=150, alpha=0.8, edgecolor='black')\n",
    "        \n",
    "        # Add current parameters if provided\n",
    "        if params and 'r' in params and 'merge_threshold' in params:\n",
    "            ax.scatter([params['r']], [params['merge_threshold']], \n",
    "                     color='red', s=250, marker='*', edgecolor='black', linewidth=2,\n",
    "                     label=f\"Current (r={params['r']}, m={params['merge_threshold']})\")\n",
    "            ax.legend(fontsize=12, loc='upper right')\n",
    "            \n",
    "        plt.colorbar(scatter, ax=ax, label=\"Number of Clusters\")\n",
    "        ax.set_title(\"Parameter Space Exploration\", fontsize=16)\n",
    "        ax.set_xlabel(\"r (similarity threshold)\", fontsize=14)\n",
    "        ax.set_ylabel(\"merge_threshold\", fontsize=14)\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add text annotations for interesting points\n",
    "        # Find min and max cluster count parameters\n",
    "        min_cluster_row = all_results.loc[all_results['num_clusters'].idxmin()]\n",
    "        max_cluster_row = all_results.loc[all_results['num_clusters'].idxmax()]\n",
    "        \n",
    "        # Add annotations\n",
    "        ax.annotate(\n",
    "            f\"Min clusters: {int(min_cluster_row['num_clusters'])}\",\n",
    "            xy=(min_cluster_row['r'], min_cluster_row['merge_threshold']),\n",
    "            xytext=(min_cluster_row['r'] - 0.05, min_cluster_row['merge_threshold'] - 0.05),\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\", color=\"black\"),\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "            fontsize=10\n",
    "        )\n",
    "        \n",
    "        ax.annotate(\n",
    "            f\"Max clusters: {int(max_cluster_row['num_clusters'])}\",\n",
    "            xy=(max_cluster_row['r'], max_cluster_row['merge_threshold']),\n",
    "            xytext=(max_cluster_row['r'] + 0.05, max_cluster_row['merge_threshold'] + 0.05),\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\", color=\"black\"),\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "            fontsize=10\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/11_parameter_space.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 10. Best Parameter Combinations Table\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        ax = plt.gca()\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create a summary table of best parameter combinations\n",
    "        best_params_df = all_results.sort_values('num_clusters').head(10)\n",
    "        best_params_df = best_params_df[['r', 'merge_threshold', 'num_clusters', \n",
    "                                     'avg_cluster_size', 'largest_cluster']]\n",
    "        \n",
    "        # Format the table with strings\n",
    "        cell_text = []\n",
    "        for _, row in best_params_df.iterrows():\n",
    "            cell_text.append([\n",
    "                f\"{row['r']:.2f}\",\n",
    "                f\"{row['merge_threshold']:.2f}\",\n",
    "                f\"{int(row['num_clusters'])}\",\n",
    "                f\"{row['avg_cluster_size']:.1f}\",\n",
    "                f\"{int(row['largest_cluster'])}\"\n",
    "            ])\n",
    "            \n",
    "        param_table = ax.table(\n",
    "            cellText=cell_text,\n",
    "            colLabels=['r', 'merge_threshold', 'num_clusters', 'avg_size', 'largest_cluster'],\n",
    "            loc='center',\n",
    "            cellLoc='center'\n",
    "        )\n",
    "        param_table.auto_set_font_size(False)\n",
    "        param_table.set_fontsize(12)\n",
    "        param_table.scale(1, 2)\n",
    "        \n",
    "        # Highlight the best parameter row\n",
    "        for i in range(len(cell_text[0])):\n",
    "            param_table[1, i].set_facecolor('#aaffaa')\n",
    "            param_table[1, i].set_text_props(weight='bold')\n",
    "        \n",
    "        plt.suptitle(\"Top 10 Parameter Combinations with Fewest Clusters\", fontsize=16, y=0.9)\n",
    "        \n",
    "        # Add a recommendation text\n",
    "        recommend_text = (\n",
    "            \"Recommendation: Choose parameters that balance the number of clusters and cluster quality.\\n\"\n",
    "            \"Fewer clusters with meaningful sizes are generally preferable for easier interpretation.\\n\"\n",
    "            \"The highlighted row represents the parameter combination with fewest clusters.\"\n",
    "        )\n",
    "        plt.figtext(0.5, 0.15, recommend_text, ha='center', fontsize=12, \n",
    "                   bbox=dict(facecolor='#f0f0f0', alpha=0.9, boxstyle='round,pad=0.5'))\n",
    "        \n",
    "        plt.savefig(f\"{output_dir}/12_best_parameters.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 11. Generate a summary HTML report\n",
    "    try:\n",
    "        # Create an HTML summary that links to all the images\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Clustering Results Summary</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
    "                h1, h2 {{ color: #333366; }}\n",
    "                .container {{ max-width: 1200px; margin: auto; background-color: white; padding: 20px; border-radius: 10px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }}\n",
    "                .img-container {{ margin: 20px 0; text-align: center; }}\n",
    "                img {{ max-width: 100%; border: 1px solid #ddd; border-radius: 5px; }}\n",
    "                .summary {{ background-color: #e6f7ff; padding: 15px; border-radius: 5px; margin-bottom: 20px; }}\n",
    "                table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}\n",
    "                th, td {{ padding: 10px; border: 1px solid #ddd; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "                a {{ color: #0066cc; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>Clustering Analysis Results</h1>\n",
    "                \n",
    "                <div class=\"summary\">\n",
    "                    <h2>Summary</h2>\n",
    "                    <p><strong>Total Samples:</strong> {total_samples}</p>\n",
    "                    <p><strong>Number of Clusters:</strong> {n_clusters}</p>\n",
    "                    <p><strong>Largest Cluster Size:</strong> {max(densities)}</p>\n",
    "                    <p><strong>Average Cluster Size:</strong> {np.mean(densities):.2f}</p>\n",
    "                    <p><strong>Median Cluster Size:</strong> {np.median(densities):.2f}</p>\n",
    "                </div>\n",
    "                \n",
    "                <h2>Visualizations</h2>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add all the images\n",
    "        images = [\n",
    "            (\"1_cluster_size_distribution.png\", \"Cluster Size Distribution\"),\n",
    "            (\"2_top_5_clusters.png\", \"Top 5 Largest Clusters\"),\n",
    "            (\"3_cumulative_distribution.png\", \"Cumulative Sample Distribution\"),\n",
    "            (\"4_similarity_heatmap.png\", \"Similarity Between Top Clusters\"),\n",
    "            (\"5_pca_visualization.png\", \"PCA Visualization\"),\n",
    "            (\"6_tsne_visualization.png\", \"t-SNE Visualization\"),\n",
    "            (\"7_cluster_quality_metrics.png\", \"Cluster Quality Metrics\"),\n",
    "            (\"8_top_5_clusters_analysis.png\", \"Detailed Analysis of Top 5 Clusters\")\n",
    "        ]\n",
    "        \n",
    "        if all_results is not None:\n",
    "            images.extend([\n",
    "                (\"9_parameter_heatmap.png\", \"Parameter Heatmap\"),\n",
    "                (\"10_parameter_impact.png\", \"Parameter Impact on Cluster Count\"),\n",
    "                (\"11_parameter_space.png\", \"Parameter Space Exploration\"),\n",
    "                (\"12_best_parameters.png\", \"Best Parameter Combinations\")\n",
    "            ])\n",
    "        \n",
    "        for img_file, img_title in images:\n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"img-container\">\n",
    "                    <h3>{img_title}</h3>\n",
    "                    <a href=\"{img_file}\" target=\"_blank\">\n",
    "                        <img src=\"{img_file}\" alt=\"{img_title}\">\n",
    "                    </a>\n",
    "                    <p><a href=\"{img_file}\" download>Download this image</a></p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Add top clusters table\n",
    "        html_content += \"\"\"\n",
    "                <h2>Top 5 Clusters</h2>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Cluster ID</th>\n",
    "                        <th>Size</th>\n",
    "                        <th>% of Total</th>\n",
    "        \"\"\"\n",
    "        \n",
    "        if similarity_matrix is not None:\n",
    "            html_content += \"\"\"\n",
    "                        <th>Avg. Intra-Similarity</th>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "                    </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add data for top 5 clusters\n",
    "        top_5_clusters = np.argsort(densities)[::-1][:5]\n",
    "        for cluster_idx in top_5_clusters:\n",
    "            cluster_size = densities[cluster_idx]\n",
    "            percentage = (cluster_size / total_samples) * 100\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td>{cluster_idx}</td>\n",
    "                        <td>{cluster_size}</td>\n",
    "                        <td>{percentage:.2f}%</td>\n",
    "            \"\"\"\n",
    "            \n",
    "            if similarity_matrix is not None:\n",
    "                # Calculate average intra-cluster similarity\n",
    "                cluster = clusters[cluster_idx]\n",
    "                if len(cluster) > 1:\n",
    "                    intra_similarities = []\n",
    "                    for idx_i in range(len(cluster)):\n",
    "                        for idx_j in range(idx_i+1, len(cluster)):\n",
    "                            intra_similarities.append(similarity_matrix[cluster[idx_i], cluster[idx_j]])\n",
    "                    avg_sim = np.mean(intra_similarities) if intra_similarities else 0\n",
    "                else:\n",
    "                    avg_sim = 1.0  # Single-element cluster\n",
    "                \n",
    "                html_content += f\"\"\"\n",
    "                        <td>{avg_sim:.4f}</td>\n",
    "                \"\"\"\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "                    </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "                </table>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Close tags\n",
    "        html_content += \"\"\"\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Write the HTML file\n",
    "        with open(f\"{output_dir}/clustering_report.html\", \"w\") as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"HTML report generated: {output_dir}/clustering_report.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating HTML report: {e}\")\n",
    "    \n",
    "    print(f\"All visualizations saved to {output_dir}/\")\n",
    "    return True\n",
    "\n",
    "# Example usage:\n",
    "visualize_clustering_detailed(\n",
    "    clusters=clusters, \n",
    "    densities=densities,\n",
    "    features_df=features_df, \n",
    "    similarity_matrix=similarity,  # Pass the similarity matrix\n",
    "    params=best_params,  # Pass the parameters used\n",
    "    all_results=all_results,  # Pass parameter exploration results\n",
    "    output_dir=\"cluster_analysis\"  # Output directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def visualize_clusters(clusters, labels, ari, nmi):\n",
    "    cluster_purities = []\n",
    "    cluster_sizes = []\n",
    "    cluster_compositions = []\n",
    "\n",
    "    for cluster in clusters:\n",
    "        true_labels = labels[cluster]\n",
    "        label_counts = Counter(true_labels)\n",
    "        most_common_label, count = label_counts.most_common(1)[0]\n",
    "        purity = count / len(cluster) * 100\n",
    "        cluster_purities.append(purity)\n",
    "        cluster_sizes.append(len(cluster))\n",
    "        cluster_compositions.append(label_counts)\n",
    "\n",
    "    # Purity Bar Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=list(range(len(clusters))), y=cluster_purities)\n",
    "    plt.ylabel('Purity (%)')\n",
    "    plt.xlabel('Cluster Index')\n",
    "    plt.title('Cluster Purity (Higher is Better)')\n",
    "    plt.show()\n",
    "\n",
    "    # Cluster Size Bar Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=list(range(len(clusters))), y=cluster_sizes)\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xlabel('Cluster Index')\n",
    "    plt.title('Cluster Sizes')\n",
    "    plt.show()\n",
    "\n",
    "    # Pie Chart of top 3 biggest clusters\n",
    "    for i in np.argsort(cluster_sizes)[-3:][::-1]:\n",
    "        labels_, counts = zip(*cluster_compositions[i].items())\n",
    "        plt.figure()\n",
    "        plt.pie(counts, labels=labels_, autopct='%1.1f%%', startangle=140)\n",
    "        plt.title(f\"Cluster {i} Label Distribution\")\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "    # Print Easy-to-Understand Summary\n",
    "    print(\"\\n📊 Clustering Evaluation Summary:\")\n",
    "    print(f\"- Adjusted Rand Index (ARI): {ari:.3f} → Low overlap with true labels.\")\n",
    "    print(f\"- Normalized Mutual Information (NMI): {nmi:.3f} → Low mutual information.\")\n",
    "    print(f\"- Average Cluster Purity: {np.mean(cluster_purities):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0e962",
   "metadata": {},
   "source": [
    "# Best Cluster result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_best_cluster_analysis(clusters, densities, features_df, similarity_matrix=None, labels=None, output_dir=\"best_cluster_analysis\"):\n",
    "    \"\"\"\n",
    "    Creates specialized visualizations focusing on the best clusters from your algorithm.\n",
    "    \n",
    "    Args:\n",
    "        clusters: List of clusters (each cluster is a list of sample indices)\n",
    "        densities: List of cluster sizes\n",
    "        features_df: DataFrame containing the original features\n",
    "        similarity_matrix: Similarity matrix used for clustering (optional)\n",
    "        labels: Optional ground truth labels for evaluation\n",
    "        output_dir: Directory to save visualizations (will be created if it doesn't exist)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import matplotlib.patches as mpatches\n",
    "    import os\n",
    "    from matplotlib.cm import get_cmap\n",
    "    from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "    import matplotlib.ticker as ticker\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "    \n",
    "    # Set overall styling for plots\n",
    "    plt.style.use('ggplot')\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'Bitstream Vera Sans', 'sans-serif']\n",
    "    \n",
    "    # Basic statistics\n",
    "    n_clusters = len(clusters)\n",
    "    total_samples = sum(densities)\n",
    "    print(f\"Analyzing best clusters out of {n_clusters} total clusters with {total_samples} total samples\")\n",
    "    \n",
    "    # Assign cluster labels to each sample\n",
    "    sample_to_cluster = np.zeros(features_df.shape[0], dtype=int) - 1  # Initialize with -1 (no cluster)\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        for sample_idx in cluster:\n",
    "            sample_to_cluster[sample_idx] = i\n",
    "    \n",
    "    # Identify top clusters based on size and quality\n",
    "    top_clusters_idx = np.argsort(densities)[::-1][:10]  # Top 10 by size\n",
    "    \n",
    "    # If similarity matrix is available, also calculate cohesion scores\n",
    "    if similarity_matrix is not None:\n",
    "        cohesion_scores = []\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            if len(cluster) > 1:\n",
    "                # Average similarity within cluster\n",
    "                cluster_sim = 0\n",
    "                count = 0\n",
    "                for idx_i in range(len(cluster)):\n",
    "                    for idx_j in range(idx_i+1, len(cluster)):\n",
    "                        cluster_sim += similarity_matrix[cluster[idx_i], cluster[idx_j]]\n",
    "                        count += 1\n",
    "                        \n",
    "                cohesion = cluster_sim / count if count > 0 else 0\n",
    "            else:\n",
    "                cohesion = 1.0  # Single-element cluster\n",
    "                \n",
    "            cohesion_scores.append(cohesion)\n",
    "        \n",
    "        # Identify top cohesive clusters (may overlap with largest)\n",
    "        top_cohesive_idx = np.argsort(cohesion_scores)[::-1][:10]\n",
    "        \n",
    "        # Calculate combined score (size * cohesion) to find \"best\" clusters\n",
    "        combined_scores = np.array(densities) * np.array(cohesion_scores)\n",
    "        combined_top_idx = np.argsort(combined_scores)[::-1][:10]\n",
    "        \n",
    "        # Make a union of all these top clusters\n",
    "        best_clusters_idx = np.unique(np.concatenate([top_clusters_idx[:5], top_cohesive_idx[:5], combined_top_idx[:5]]))\n",
    "    else:\n",
    "        # Without similarity matrix, just use size\n",
    "        best_clusters_idx = top_clusters_idx[:10]\n",
    "    \n",
    "    # =============== 1. Overview of Best Clusters ===============\n",
    "    # Create a summary visualization of the best clusters\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n",
    "    ax2 = plt.subplot2grid((2, 2), (1, 0))\n",
    "    ax3 = plt.subplot2grid((2, 2), (1, 1))\n",
    "    \n",
    "    # Bar chart of best cluster sizes\n",
    "    colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(best_clusters_idx)))\n",
    "    bars = ax1.bar(best_clusters_idx, [densities[i] for i in best_clusters_idx], color=colors)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height + 5,\n",
    "            f'{int(height)}',\n",
    "            ha='center', \n",
    "            va='bottom',\n",
    "            fontsize=12,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    # Add percentage labels inside bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        percentage = (height / total_samples) * 100\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height / 2,\n",
    "            f'{percentage:.1f}%',\n",
    "            ha='center', \n",
    "            va='center',\n",
    "            fontsize=11,\n",
    "            fontweight='bold',\n",
    "            color='white'\n",
    "        )\n",
    "    \n",
    "    ax1.set_title(\"Best Clusters by Size\", fontsize=16)\n",
    "    ax1.set_xlabel(\"Cluster ID\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Number of Samples\", fontsize=14)\n",
    "    \n",
    "    # Calculate total percentage covered by best clusters\n",
    "    best_total = sum(densities[i] for i in best_clusters_idx)\n",
    "    best_percent = (best_total / total_samples) * 100\n",
    "    ax1.text(\n",
    "        0.5, 0.95,\n",
    "        f\"These {len(best_clusters_idx)} clusters contain {best_percent:.1f}% of all samples\",\n",
    "        transform=ax1.transAxes,\n",
    "        ha='center',\n",
    "        fontsize=13,\n",
    "        fontweight='bold',\n",
    "        bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.5')\n",
    "    )\n",
    "    \n",
    "    # Pie chart showing proportion of samples in best clusters vs other clusters\n",
    "    sizes = [best_total, total_samples - best_total]\n",
    "    labels = [f'Best Clusters ({best_percent:.1f}%)', f'Other Clusters ({100-best_percent:.1f}%)']\n",
    "    colors = ['#3498db', '#e74c3c']\n",
    "    \n",
    "    ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', \n",
    "           startangle=90, shadow=True, explode=(0.1, 0))\n",
    "    ax2.set_title(\"Distribution of Samples\", fontsize=14)\n",
    "    \n",
    "    # Scatter plot of cluster size vs cohesion (if available)\n",
    "    if similarity_matrix is not None:\n",
    "        ax3.scatter(densities, cohesion_scores, s=100, alpha=0.7, c=range(len(densities)), cmap='viridis')\n",
    "        \n",
    "        # Highlight best clusters\n",
    "        for idx in best_clusters_idx:\n",
    "            ax3.scatter(densities[idx], cohesion_scores[idx], s=200, edgecolor='red', \n",
    "                      facecolor='none', linewidth=2, marker='o')\n",
    "            ax3.text(densities[idx]+5, cohesion_scores[idx], f'{idx}', fontsize=12)\n",
    "        \n",
    "        ax3.set_title(\"Cluster Size vs. Cohesion\", fontsize=14)\n",
    "        ax3.set_xlabel(\"Cluster Size\", fontsize=12)\n",
    "        ax3.set_ylabel(\"Cohesion Score\", fontsize=12)\n",
    "        \n",
    "        # Add quadrant labels for interpretation\n",
    "        ax3.axhline(np.median(cohesion_scores), color='gray', linestyle='--', alpha=0.7)\n",
    "        ax3.axvline(np.median(densities), color='gray', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add quadrant annotations\n",
    "        quadrants = [\n",
    "            (0.25, 0.25, \"Small\\nLow Cohesion\"),\n",
    "            (0.25, 0.75, \"Small\\nHigh Cohesion\"),\n",
    "            (0.75, 0.25, \"Large\\nLow Cohesion\"),\n",
    "            (0.75, 0.75, \"Large\\nHigh Cohesion\")\n",
    "        ]\n",
    "        \n",
    "        for x, y, text in quadrants:\n",
    "            ax3.text(\n",
    "                x, y, text,\n",
    "                transform=ax3.transAxes,\n",
    "                ha='center', va='center',\n",
    "                fontsize=10, alpha=0.7,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.6)\n",
    "            )\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, \"Similarity matrix not provided\", \n",
    "                ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/1_best_clusters_overview.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # =============== 2. Dimensionality Reduction: Best Clusters Only ===============\n",
    "    # Convert features to numpy for dimensionality reduction\n",
    "    X = features_df.values\n",
    "    \n",
    "    # Apply PCA to visualize best clusters\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Apply PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Get a colormap with distinct colors\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(best_clusters_idx)))\n",
    "    \n",
    "    # Create list for legend\n",
    "    legend_elements = []\n",
    "    \n",
    "    # Plot unclustered or non-best cluster points first\n",
    "    mask = np.ones(len(sample_to_cluster), dtype=bool)\n",
    "    for idx in best_clusters_idx:\n",
    "        cluster_samples = np.where(sample_to_cluster == idx)[0]\n",
    "        mask[cluster_samples] = False\n",
    "    \n",
    "    if np.any(mask):\n",
    "        ax1.scatter(\n",
    "            X_pca[mask, 0], \n",
    "            X_pca[mask, 1], \n",
    "            c='lightgray', \n",
    "            alpha=0.3, \n",
    "            s=20,\n",
    "            label=\"Other clusters\"\n",
    "        )\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                     markerfacecolor='lightgray', \n",
    "                                     label=f'Other clusters', \n",
    "                                     markersize=8))\n",
    "    \n",
    "    # Plot each best cluster with distinct color\n",
    "    for i, cluster_idx in enumerate(best_clusters_idx):\n",
    "        cluster_samples = np.where(sample_to_cluster == cluster_idx)[0]\n",
    "        ax1.scatter(\n",
    "            X_pca[cluster_samples, 0], \n",
    "            X_pca[cluster_samples, 1], \n",
    "            c=[colors[i]], \n",
    "            alpha=0.7, \n",
    "            s=50,\n",
    "            label=f\"Cluster {cluster_idx} (size: {densities[cluster_idx]})\"\n",
    "        )\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                     markerfacecolor=colors[i], \n",
    "                                     label=f'Cluster {cluster_idx} (size: {densities[cluster_idx]})', \n",
    "                                     markersize=8))\n",
    "    \n",
    "    # Add variance explained\n",
    "    var_explained = pca.explained_variance_ratio_\n",
    "    ax1.set_xlabel(f\"PC1 ({var_explained[0]:.2%} variance)\", fontsize=14)\n",
    "    ax1.set_ylabel(f\"PC2 ({var_explained[1]:.2%} variance)\", fontsize=14)\n",
    "    ax1.set_title(\"PCA: Best Clusters\", fontsize=16)\n",
    "    \n",
    "    # Add legend with better positioning\n",
    "    ax1.legend(\n",
    "        handles=legend_elements, \n",
    "        title=\"Clusters\", \n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        fontsize=10\n",
    "    )\n",
    "    \n",
    "    # t-SNE visualization of best clusters\n",
    "    try:\n",
    "        # Limit samples for t-SNE if too many\n",
    "        max_tsne_samples = 5000\n",
    "        if X.shape[0] > max_tsne_samples:\n",
    "            np.random.seed(42)\n",
    "            sample_indices = np.random.choice(X.shape[0], max_tsne_samples, replace=False)\n",
    "            X_subset = X[sample_indices]\n",
    "            clusters_subset = sample_to_cluster[sample_indices]\n",
    "        else:\n",
    "            X_subset = X\n",
    "            clusters_subset = sample_to_cluster\n",
    "        \n",
    "        # Apply t-SNE with good parameters\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=min(40, len(X_subset)-1), \n",
    "            n_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        X_tsne = tsne.fit_transform(X_subset)\n",
    "        \n",
    "        # Create mask for best clusters in the t-SNE subset\n",
    "        mask_tsne = np.ones(len(clusters_subset), dtype=bool)\n",
    "        legend_elements_tsne = []\n",
    "        \n",
    "        # Plot background points\n",
    "        for idx in best_clusters_idx:\n",
    "            cluster_samples = np.where(clusters_subset == idx)[0]\n",
    "            mask_tsne[cluster_samples] = False\n",
    "        \n",
    "        if np.any(mask_tsne):\n",
    "            ax2.scatter(\n",
    "                X_tsne[mask_tsne, 0], \n",
    "                X_tsne[mask_tsne, 1], \n",
    "                c='lightgray', \n",
    "                alpha=0.3, \n",
    "                s=20,\n",
    "                label=\"Other clusters\"\n",
    "            )\n",
    "            legend_elements_tsne.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                         markerfacecolor='lightgray', \n",
    "                                         label=f'Other clusters', \n",
    "                                         markersize=8))\n",
    "        \n",
    "        # Plot best clusters in t-SNE space\n",
    "        for i, cluster_idx in enumerate(best_clusters_idx):\n",
    "            cluster_samples = np.where(clusters_subset == cluster_idx)[0]\n",
    "            if len(cluster_samples) > 0:\n",
    "                ax2.scatter(\n",
    "                    X_tsne[cluster_samples, 0], \n",
    "                    X_tsne[cluster_samples, 1], \n",
    "                    c=[colors[i]], \n",
    "                    alpha=0.7, \n",
    "                    s=50,\n",
    "                    label=f\"Cluster {cluster_idx}\"\n",
    "                )\n",
    "                legend_elements_tsne.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                             markerfacecolor=colors[i], \n",
    "                                             label=f'Cluster {cluster_idx}', \n",
    "                                             markersize=8))\n",
    "        \n",
    "        ax2.set_title(\"t-SNE: Best Clusters\", fontsize=16)\n",
    "        \n",
    "        # Add legend\n",
    "        ax2.legend(\n",
    "            handles=legend_elements_tsne, \n",
    "            title=\"Clusters\", \n",
    "            loc=\"upper right\",\n",
    "            bbox_to_anchor=(1.05, 1),\n",
    "            fontsize=10\n",
    "        )\n",
    "    except Exception as e:\n",
    "        ax2.text(0.5, 0.5, f\"t-SNE error: {str(e)}\", \n",
    "                ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/2_best_clusters_visualization.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # =============== 3. Feature Analysis for Best Clusters ===============\n",
    "    if features_df.shape[1] > 0:  # Make sure we have features\n",
    "        # Calculate which features best discriminate the top clusters\n",
    "        # Convert to DataFrame for easier analysis\n",
    "        X_df = features_df.copy()\n",
    "        X_df['cluster'] = sample_to_cluster\n",
    "        \n",
    "        # Filter to best clusters only\n",
    "        best_clusters_mask = np.isin(X_df['cluster'], best_clusters_idx)\n",
    "        X_best = X_df[best_clusters_mask]\n",
    "        \n",
    "        # For each feature, calculate difference between clusters\n",
    "        feature_importance = {}\n",
    "        importance_data = []\n",
    "        \n",
    "        # Limit to 20 features for visualization\n",
    "        feature_cols = features_df.columns[:20] if len(features_df.columns) > 20 else features_df.columns\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            # Skip if not numeric\n",
    "            if not np.issubdtype(X_df[col].dtype, np.number):\n",
    "                continue\n",
    "                \n",
    "            # Calculate mean value for each cluster\n",
    "            cluster_means = {}\n",
    "            for cluster_idx in best_clusters_idx:\n",
    "                cluster_data = X_best[X_best['cluster'] == cluster_idx][col]\n",
    "                if len(cluster_data) > 0:\n",
    "                    cluster_means[cluster_idx] = cluster_data.mean()\n",
    "            \n",
    "            # Skip if any cluster has no data\n",
    "            if len(cluster_means) < len(best_clusters_idx):\n",
    "                continue\n",
    "                \n",
    "            # Calculate variance ratio: between-cluster variance / within-cluster variance\n",
    "            global_mean = X_best[col].mean()\n",
    "            \n",
    "            # Between-cluster variance\n",
    "            between_var = sum((cluster_means[idx] - global_mean) ** 2 for idx in cluster_means) / len(cluster_means)\n",
    "            \n",
    "            # Within-cluster variance\n",
    "            within_var_total = 0\n",
    "            within_var_count = 0\n",
    "            \n",
    "            for cluster_idx in best_clusters_idx:\n",
    "                cluster_data = X_best[X_best['cluster'] == cluster_idx][col]\n",
    "                if len(cluster_data) > 1:  # Need at least 2 points for variance\n",
    "                    within_var_total += np.var(cluster_data) * (len(cluster_data) - 1)\n",
    "                    within_var_count += len(cluster_data) - 1\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if within_var_count > 0 and within_var_total > 0:\n",
    "                within_var = within_var_total / within_var_count\n",
    "                f_ratio = between_var / within_var\n",
    "            else:\n",
    "                f_ratio = 0\n",
    "                \n",
    "            feature_importance[col] = f_ratio\n",
    "            \n",
    "            # Store data for visualization\n",
    "            for cluster_idx in best_clusters_idx:\n",
    "                cluster_data = X_best[X_best['cluster'] == cluster_idx][col]\n",
    "                if len(cluster_data) > 0:\n",
    "                    importance_data.append({\n",
    "                        'Feature': col,\n",
    "                        'Cluster': f'Cluster {cluster_idx}',\n",
    "                        'Value': cluster_data.mean(),\n",
    "                        'F-ratio': f_ratio\n",
    "                    })\n",
    "        \n",
    "        # Convert to DataFrame for easier plotting\n",
    "        importance_df = pd.DataFrame(importance_data)\n",
    "        \n",
    "        # Get top 10 most discriminative features\n",
    "        top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        top_feature_names = [f[0] for f in top_features]\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n",
    "        ax2 = plt.subplot2grid((2, 2), (1, 0), colspan=2)\n",
    "        \n",
    "        # Feature importance bar chart\n",
    "        feature_imp_values = [f[1] for f in top_features]\n",
    "        feature_names = [str(f[0])[:20] for f in top_features]  # Truncate long names\n",
    "        \n",
    "        bars = ax1.bar(feature_names, feature_imp_values, color='royalblue')\n",
    "        \n",
    "        # Add values on top of bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(\n",
    "                bar.get_x() + bar.get_width()/2.,\n",
    "                height + 0.1,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', \n",
    "                va='bottom',\n",
    "                fontsize=10\n",
    "            )\n",
    "        \n",
    "        ax1.set_title(\"Top 10 Most Discriminative Features\", fontsize=16)\n",
    "        ax1.set_xlabel(\"Feature\", fontsize=14)\n",
    "        ax1.set_ylabel(\"F-ratio (Between/Within Variance)\", fontsize=14)\n",
    "        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Feature values across clusters heatmap\n",
    "        if importance_df.empty or len(top_feature_names) == 0:\n",
    "            ax2.text(0.5, 0.5, \"Not enough data for feature analysis\", \n",
    "                    ha='center', va='center', fontsize=14)\n",
    "        else:\n",
    "            # Filter to top features\n",
    "            plot_df = importance_df[importance_df['Feature'].isin(top_feature_names)]\n",
    "            \n",
    "            if not plot_df.empty:\n",
    "                # Create pivot table\n",
    "                pivot = plot_df.pivot_table(\n",
    "                    index='Feature', \n",
    "                    columns='Cluster', \n",
    "                    values='Value',\n",
    "                    aggfunc='mean'\n",
    "                )\n",
    "                \n",
    "                # Normalize for better visualization\n",
    "                pivot_norm = (pivot - pivot.min()) / (pivot.max() - pivot.min())\n",
    "                \n",
    "                # Plot heatmap\n",
    "                sns.heatmap(\n",
    "                    pivot_norm, \n",
    "                    annot=True, \n",
    "                    cmap=\"YlGnBu\", \n",
    "                    ax=ax2, \n",
    "                    fmt='.2f', \n",
    "                    cbar_kws={'label': 'Normalized Value'}\n",
    "                )\n",
    "                \n",
    "                ax2.set_title(\"Feature Values Across Best Clusters (Normalized)\", fontsize=16)\n",
    "                ax2.set_ylabel(\"Feature\", fontsize=14)\n",
    "                ax2.set_xlabel(\"Cluster\", fontsize=14)\n",
    "            else:\n",
    "                ax2.text(0.5, 0.5, \"Not enough data for feature analysis\", \n",
    "                        ha='center', va='center', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/3_feature_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # =============== 4. Feature Distribution Within Best Clusters ===============\n",
    "        # Create violin plots for top 5 most important features across clusters\n",
    "        if len(top_feature_names) > 0:\n",
    "            top_5_features = top_feature_names[:5]\n",
    "            \n",
    "            plt.figure(figsize=(16, 12))\n",
    "            \n",
    "            for i, feature in enumerate(top_5_features):\n",
    "                ax = plt.subplot(3, 2, i+1)\n",
    "                \n",
    "                # Data for plot\n",
    "                plot_data = []\n",
    "                plot_labels = []\n",
    "                \n",
    "                for cluster_idx in best_clusters_idx:\n",
    "                    cluster_data = X_df[X_df['cluster'] == cluster_idx][feature].values\n",
    "                    if len(cluster_data) > 0:\n",
    "                        plot_data.append(cluster_data)\n",
    "                        plot_labels.append(f'Cluster {cluster_idx}')\n",
    "                \n",
    "                if len(plot_data) > 0:\n",
    "                    # Create violin plot\n",
    "                    parts = ax.violinplot(\n",
    "                        plot_data, \n",
    "                        showmeans=True, \n",
    "                        showmedians=True,\n",
    "                        showextrema=True\n",
    "                    )\n",
    "                    \n",
    "                    # Customize violin plots\n",
    "                    for pc, color in zip(parts['bodies'], colors[:len(plot_data)]):\n",
    "                        pc.set_facecolor(color)\n",
    "                        pc.set_alpha(0.7)\n",
    "                    \n",
    "                    # Set labels and title\n",
    "                    ax.set_xticks(range(1, len(plot_labels) + 1))\n",
    "                    ax.set_xticklabels(plot_labels, rotation=45, ha='right')\n",
    "                    ax.set_title(f\"Distribution of {feature}\", fontsize=14)\n",
    "                    ax.set_ylabel(\"Feature Value\", fontsize=12)\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, \"Not enough data for feature analysis\", \n",
    "                           ha='center', va='center', fontsize=14)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/4_feature_distributions.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    # =============== 5. Cluster Similarity Analysis ===============\n",
    "    if similarity_matrix is not None:\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n",
    "        ax2 = plt.subplot2grid((2, 2), (1, 0), colspan=2)\n",
    "        \n",
    "        # Calculate similarity between best clusters\n",
    "        best_cluster_similarity = np.zeros((len(best_clusters_idx), len(best_clusters_idx)))\n",
    "        \n",
    "        for i, cluster_i_idx in enumerate(best_clusters_idx):\n",
    "            for j, cluster_j_idx in enumerate(best_clusters_idx):\n",
    "                cluster_i = clusters[cluster_i_idx]\n",
    "                cluster_j = clusters[cluster_j_idx]\n",
    "                \n",
    "                # Get all pairwise similarities between samples in the two clusters\n",
    "                similarities = []\n",
    "                for idx_i in cluster_i:\n",
    "                    for idx_j in cluster_j:\n",
    "                        similarities.append(similarity_matrix[idx_i, idx_j])\n",
    "                \n",
    "                # Calculate average similarity\n",
    "                best_cluster_similarity[i, j] = np.mean(similarities) if similarities else 0\n",
    "        \n",
    "        # Create custom labels showing cluster index and size\n",
    "        labels = [f\"{idx}\\n(size: {densities[idx]})\" for idx in best_clusters_idx]\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(\n",
    "            best_cluster_similarity, \n",
    "            annot=True, \n",
    "            cmap=\"YlGnBu\", \n",
    "            ax=ax1, \n",
    "            vmin=0, \n",
    "            vmax=1, \n",
    "            fmt='.2f', \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels,\n",
    "            linewidths=0.5,\n",
    "            annot_kws={\"size\": 10, \"weight\": \"bold\"}\n",
    "        )\n",
    "        \n",
    "        ax1.set_title(\"Similarity Between Best Clusters\", fontsize=16)\n",
    "        ax1.set_xlabel(\"Cluster ID (size)\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Cluster ID (size)\", fontsize=14)\n",
    "        \n",
    "        # Create hierarchical clustering of the clusters based on similarity\n",
    "        # Convert similarity to distance (1 - similarity)\n",
    "        distance_matrix = 1 - best_cluster_similarity\n",
    "        \n",
    "        # Compute linkage\n",
    "        Z = linkage(distance_matrix, method='ward')\n",
    "        \n",
    "        # Plot dendrogram\n",
    "        dendrogram(\n",
    "            Z, \n",
    "            labels=[f\"Cluster {idx}\" for idx in best_clusters_idx],\n",
    "            ax=ax2, \n",
    "            orientation='top',\n",
    "            leaf_font_size=12,\n",
    "            color_threshold=0.5  # adjust this threshold as needed\n",
    "        )\n",
    "        \n",
    "        ax2.set_title(\"Hierarchical Clustering of Best Clusters\", fontsize=16)\n",
    "        ax2.set_ylabel(\"Distance (1 - Similarity)\", fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/5_cluster_similarity.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # =============== 6. Silhouette Analysis of Best Clusters ===============\n",
    "    if similarity_matrix is not None:\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Create a filter for samples in best clusters\n",
    "        best_clusters_mask = np.isin(sample_to_cluster, best_clusters_idx)\n",
    "        best_cluster_samples = np.where(best_clusters_mask)[0]\n",
    "        \n",
    "        if len(best_cluster_samples) > 1:\n",
    "            # Convert similarity to distance for silhouette\n",
    "            distance_matrix = 1 - similarity_matrix\n",
    "            \n",
    "            # Filter matrices for best clusters only\n",
    "            best_distances = distance_matrix[best_cluster_samples][:, best_cluster_samples]\n",
    "            best_labels = sample_to_cluster[best_cluster_samples]\n",
    "            \n",
    "            # Compute silhouette scores\n",
    "            try:\n",
    "                silhouette_vals = silhouette_samples(\n",
    "                    best_distances, \n",
    "                    best_labels, \n",
    "                    metric='precomputed'\n",
    "                )\n",
    "                \n",
    "                # Compute average silhouette score\n",
    "                avg_silhouette = silhouette_score(\n",
    "                    best_distances, \n",
    "                    best_labels, \n",
    "                    metric='precomputed'\n",
    "                )\n",
    "                \n",
    "                # Plot silhouette\n",
    "                y_lower = 10\n",
    "                ax = plt.gca()\n",
    "                \n",
    "                # Sort clusters by size (largest first)\n",
    "                cluster_sizes = {}\n",
    "                for idx in best_clusters_idx:\n",
    "                    cluster_sizes[idx] = np.sum(best_labels == idx)\n",
    "                    \n",
    "                sorted_clusters = sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "                sorted_idx = [item[0] for item in sorted_clusters]\n",
    "                \n",
    "                # Plot each cluster's silhouette\n",
    "                for i, cluster_idx in enumerate(sorted_idx):\n",
    "                    # Get silhouette values for this cluster\n",
    "                    cluster_silhouette_vals = silhouette_vals[best_labels == cluster_idx]\n",
    "                    cluster_silhouette_vals.sort()\n",
    "                    \n",
    "                    size = cluster_silhouette_vals.shape[0]\n",
    "                    if size == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    y_upper = y_lower + size\n",
    "                    \n",
    "                    color = plt.cm.nipy_spectral(i / len(sorted_idx))\n",
    "                    ax.fill_betweenx(\n",
    "                        np.arange(y_lower, y_upper),\n",
    "                        0, \n",
    "                        cluster_silhouette_vals,\n",
    "                        facecolor=color, \n",
    "                        edgecolor=color, \n",
    "                        alpha=0.7\n",
    "                    )\n",
    "                    \n",
    "                    # Label cluster\n",
    "                    ax.text(\n",
    "                        -0.05, \n",
    "                        y_lower + 0.5 * size, \n",
    "                        f'Cluster {cluster_idx}\\n({size} samples)',\n",
    "                        fontsize=10, \n",
    "                        verticalalignment='center'\n",
    "                    )\n",
    "                    \n",
    "                    # Compute next y_lower\n",
    "                    y_lower = y_upper + 10\n",
    "                \n",
    "                # Add average silhouette line\n",
    "                ax.axvline(\n",
    "                    x=avg_silhouette, \n",
    "                    color=\"red\", \n",
    "                    linestyle=\"--\",\n",
    "                    label=f'Average: {avg_silhouette:.3f}'\n",
    "                )\n",
    "                \n",
    "                ax.set_title(\"Silhouette Analysis of Best Clusters\", fontsize=16)\n",
    "                ax.set_xlabel(\"Silhouette Coefficient\", fontsize=14)\n",
    "                ax.set_ylabel(\"Cluster\", fontsize=14)\n",
    "                \n",
    "                # Set limits\n",
    "                ax.set_xlim([-0.1, 1])\n",
    "                \n",
    "                # Add legend\n",
    "                ax.legend(loc='best', fontsize=12)\n",
    "                # Add interpretive labels\n",
    "                silhouette_ranges = [\n",
    "                    (-0.1, 0.25, \"Poor separation\", \"lightcoral\"),\n",
    "                    (0.25, 0.5, \"Fair separation\", \"khaki\"),\n",
    "                    (0.5, 0.75, \"Good separation\", \"palegreen\"),\n",
    "                    (0.75, 1.0, \"Excellent separation\", \"lightblue\")\n",
    "                ]\n",
    "                \n",
    "                for start, end, label, color in silhouette_ranges:\n",
    "                    ax.axvspan(start, end, alpha=0.2, color=color)\n",
    "                    ax.text(\n",
    "                        (start + end) / 2, \n",
    "                        -0.05, \n",
    "                        label, \n",
    "                        ha='center', \n",
    "                        va='top', \n",
    "                        fontsize=10,\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7)\n",
    "                    )\n",
    "                \n",
    "            except Exception as e:\n",
    "                plt.clf()  # Clear the figure\n",
    "                ax = plt.gca()\n",
    "                ax.text(0.5, 0.5, f\"Error computing silhouette: {str(e)}\", \n",
    "                       ha='center', va='center', fontsize=14)\n",
    "        else:\n",
    "            ax = plt.gca()\n",
    "            ax.text(0.5, 0.5, \"Not enough samples for silhouette analysis\", \n",
    "                   ha='center', va='center', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/6_silhouette_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # =============== 7. Cluster Stability Analysis ===============\n",
    "    # This simulates how stable your clusters are by sampling\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n",
    "    ax2 = plt.subplot2grid((2, 2), (1, 0), colspan=1)\n",
    "    ax3 = plt.subplot2grid((2, 2), (1, 1), colspan=1)\n",
    "    \n",
    "    # Simulate stability by subsampling\n",
    "    n_iterations = 20\n",
    "    stability_scores = {idx: [] for idx in best_clusters_idx}\n",
    "    overlap_scores = []\n",
    "    \n",
    "    try:\n",
    "        # Subsampling fraction (e.g., 80% of data)\n",
    "        subsample_fraction = 0.8\n",
    "        \n",
    "        for _ in range(n_iterations):\n",
    "            # Subsample the data\n",
    "            n_samples = features_df.shape[0]\n",
    "            subsample_size = int(n_samples * subsample_fraction)\n",
    "            \n",
    "            # Create random subsample\n",
    "            np.random.seed(_ + 42)  # Different seed each time\n",
    "            subsample_indices = np.random.choice(n_samples, subsample_size, replace=False)\n",
    "            \n",
    "            # Calculate overlap of subsample with each best cluster\n",
    "            for cluster_idx in best_clusters_idx:\n",
    "                cluster = clusters[cluster_idx]\n",
    "                \n",
    "                # Calculate how many samples from the cluster are in the subsample\n",
    "                overlap = np.intersect1d(cluster, subsample_indices)\n",
    "                \n",
    "                # Calculate proportion of cluster preserved in subsample\n",
    "                if len(cluster) > 0:\n",
    "                    stability = len(overlap) / len(cluster)\n",
    "                else:\n",
    "                    stability = 0\n",
    "                    \n",
    "                stability_scores[cluster_idx].append(stability)\n",
    "            \n",
    "            # Calculate overall average overlap across all best clusters\n",
    "            overlap_scores.append(np.mean([stability_scores[idx][-1] for idx in best_clusters_idx]))\n",
    "        \n",
    "        # Plot stability scores\n",
    "        stability_means = [np.mean(stability_scores[idx]) for idx in best_clusters_idx]\n",
    "        stability_stds = [np.std(stability_scores[idx]) for idx in best_clusters_idx]\n",
    "        \n",
    "        # Bar chart of stability\n",
    "        bars = ax1.bar(\n",
    "            [f\"Cluster {idx}\" for idx in best_clusters_idx], \n",
    "            stability_means, \n",
    "            yerr=stability_stds,\n",
    "            capsize=5,\n",
    "            color=colors[:len(best_clusters_idx)]\n",
    "        )\n",
    "        \n",
    "        # Add values on top of bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(\n",
    "                bar.get_x() + bar.get_width()/2.,\n",
    "                height + 0.02,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', \n",
    "                va='bottom',\n",
    "                fontsize=10\n",
    "            )\n",
    "        \n",
    "        ax1.set_title(\"Cluster Stability Analysis\", fontsize=16)\n",
    "        ax1.set_xlabel(\"Cluster\", fontsize=14)\n",
    "        ax1.set_ylabel(\"Stability Score (Higher is Better)\", fontsize=14)\n",
    "        ax1.set_ylim(0, 1.1)\n",
    "        \n",
    "        # Add interpretive thresholds\n",
    "        ax1.axhline(0.9, color='green', linestyle='--', alpha=0.7, label='Excellent')\n",
    "        ax1.axhline(0.7, color='orange', linestyle='--', alpha=0.7, label='Good')\n",
    "        ax1.axhline(0.5, color='red', linestyle='--', alpha=0.7, label='Fair')\n",
    "        ax1.legend(fontsize=12)\n",
    "        \n",
    "        # Plot histogram of stability scores for the two most stable clusters\n",
    "        sorted_stability = sorted([(idx, np.mean(scores)) for idx, scores in stability_scores.items()], \n",
    "                                 key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if len(sorted_stability) >= 1:\n",
    "            most_stable_idx = sorted_stability[0][0]\n",
    "            \n",
    "            ax2.hist(\n",
    "                stability_scores[most_stable_idx], \n",
    "                bins=10, \n",
    "                alpha=0.7, \n",
    "                color=colors[0],\n",
    "                edgecolor='black'\n",
    "            )\n",
    "            \n",
    "            ax2.set_title(f\"Stability Distribution: Cluster {most_stable_idx}\", fontsize=14)\n",
    "            ax2.set_xlabel(\"Stability Score\", fontsize=12)\n",
    "            ax2.set_ylabel(\"Frequency\", fontsize=12)\n",
    "            ax2.set_xlim(0, 1)\n",
    "        \n",
    "        # Plot trend of overall overlap across iterations\n",
    "        ax3.plot(\n",
    "            range(1, n_iterations + 1),\n",
    "            overlap_scores,\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            color='royalblue',\n",
    "            linewidth=2\n",
    "        )\n",
    "        \n",
    "        ax3.set_title(\"Overall Cluster Stability Trend\", fontsize=14)\n",
    "        ax3.set_xlabel(\"Iteration\", fontsize=12)\n",
    "        ax3.set_ylabel(\"Average Stability\", fontsize=12)\n",
    "        ax3.set_ylim(0, 1)\n",
    "        ax3.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "    except Exception as e:\n",
    "        for ax in [ax1, ax2, ax3]:\n",
    "            ax.text(0.5, 0.5, f\"Error in stability analysis: {str(e)}\", \n",
    "                   ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/7_stability_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # =============== 8. Top 5 Best Clusters Detail Cards ===============\n",
    "    # Create detailed \"profile cards\" for the top 5 best clusters\n",
    "    \n",
    "    # Get top 5 clusters by combined score (size * cohesion)\n",
    "    if similarity_matrix is not None and len(cohesion_scores) > 0:\n",
    "        combined_scores = np.array(densities) * np.array(cohesion_scores)\n",
    "        top5_idx = np.argsort(combined_scores)[::-1][:5]\n",
    "    else:\n",
    "        # Without similarity, just use size\n",
    "        top5_idx = np.argsort(densities)[::-1][:5]\n",
    "    \n",
    "    # Create a detail card for each top cluster\n",
    "    plt.figure(figsize=(20, 25))\n",
    "    \n",
    "    # Use gridspec for more control\n",
    "    gs = gridspec.GridSpec(5, 1, height_ratios=[1, 1, 1, 1, 1], hspace=0.4)\n",
    "    \n",
    "    for i, cluster_idx in enumerate(top5_idx):\n",
    "        # Create a subplot for this cluster\n",
    "        ax_main = plt.subplot(gs[i])\n",
    "        \n",
    "        # Calculate cluster stats\n",
    "        cluster = clusters[cluster_idx]\n",
    "        cluster_size = len(cluster)\n",
    "        percentage = (cluster_size / total_samples) * 100\n",
    "        \n",
    "        # Get cohesion score if available\n",
    "        if similarity_matrix is not None and i < len(cohesion_scores):\n",
    "            cohesion = cohesion_scores[cluster_idx]\n",
    "        else:\n",
    "            cohesion = None\n",
    "        \n",
    "        # Create a profile card layout using gridspec\n",
    "        # This creates a card with stats and visualizations\n",
    "        ax_main.axis('off')\n",
    "        \n",
    "        # Use a light background for the card\n",
    "        ax_main.add_patch(plt.Rectangle(\n",
    "            (0, 0), 1, 1, \n",
    "            transform=ax_main.transAxes,\n",
    "            facecolor='#f5f5f5', \n",
    "            edgecolor='#cccccc',\n",
    "            linewidth=2,\n",
    "            zorder=-1\n",
    "        ))\n",
    "        \n",
    "        # Add cluster title\n",
    "        ax_main.text(\n",
    "            0.5, 0.95,\n",
    "            f\"Cluster {cluster_idx} Profile\",\n",
    "            transform=ax_main.transAxes,\n",
    "            ha='center',\n",
    "            va='top',\n",
    "            fontsize=18,\n",
    "            fontweight='bold',\n",
    "            bbox=dict(facecolor='#3498db', alpha=0.7, boxstyle='round,pad=0.3', edgecolor='none'),\n",
    "            color='white'\n",
    "        )\n",
    "        \n",
    "        # Add basic stats\n",
    "        stats_text = (\n",
    "            f\"Size: {cluster_size} samples ({percentage:.1f}% of total)\\n\"\n",
    "        )\n",
    "        \n",
    "        if cohesion is not None:\n",
    "            stats_text += f\"Cohesion: {cohesion:.3f}\\n\"\n",
    "            \n",
    "            # Add qualitative assessment of cohesion\n",
    "            if cohesion > 0.8:\n",
    "                cohesion_quality = \"Excellent (highly similar samples)\"\n",
    "            elif cohesion > 0.6:\n",
    "                cohesion_quality = \"Good (mostly similar samples)\"\n",
    "            elif cohesion > 0.4:\n",
    "                cohesion_quality = \"Moderate (somewhat similar samples)\"\n",
    "            else:\n",
    "                cohesion_quality = \"Low (diverse samples)\"\n",
    "                \n",
    "            stats_text += f\"Cohesion Quality: {cohesion_quality}\\n\"\n",
    "        \n",
    "        # Add stability if calculated\n",
    "        if cluster_idx in stability_scores and stability_scores[cluster_idx]:\n",
    "            stability = np.mean(stability_scores[cluster_idx])\n",
    "            stats_text += f\"Stability: {stability:.3f}\\n\"\n",
    "            \n",
    "            # Add qualitative assessment of stability\n",
    "            if stability > 0.9:\n",
    "                stability_quality = \"Excellent (very stable cluster)\"\n",
    "            elif stability > 0.7:\n",
    "                stability_quality = \"Good (stable cluster)\"\n",
    "            elif stability > 0.5:\n",
    "                stability_quality = \"Moderate (somewhat stable cluster)\"\n",
    "            else:\n",
    "                stability_quality = \"Low (unstable cluster)\"\n",
    "                \n",
    "            stats_text += f\"Stability Quality: {stability_quality}\\n\"\n",
    "        \n",
    "        # Add feature importance if calculated\n",
    "        if 'importance_df' in locals() and not importance_df.empty:\n",
    "            # Get top 3 features for this cluster\n",
    "            cluster_features = importance_df[importance_df['Cluster'] == f'Cluster {cluster_idx}']\n",
    "            if not cluster_features.empty:\n",
    "                top_cluster_features = cluster_features.sort_values('F-ratio', ascending=False).head(3)\n",
    "                \n",
    "                if not top_cluster_features.empty:\n",
    "                    stats_text += \"\\nKey Features:\\n\"\n",
    "                    for _, row in top_cluster_features.iterrows():\n",
    "                        stats_text += f\"- {row['Feature']}: {row['Value']:.3f}\\n\"\n",
    "        \n",
    "        # Add the stats text\n",
    "        ax_main.text(\n",
    "            0.05, 0.7,\n",
    "            stats_text,\n",
    "            transform=ax_main.transAxes,\n",
    "            ha='left',\n",
    "            va='top',\n",
    "            fontsize=14,\n",
    "            bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3', edgecolor='#cccccc')\n",
    "        )\n",
    "        \n",
    "        # Add a mini PCA plot for this cluster\n",
    "        if X.shape[0] > 0 and X.shape[1] > 1:\n",
    "            # Create a mini-axis for PCA\n",
    "            ax_pca = plt.axes([0.6, 0.25 + i*0.2, 0.3, 0.15])\n",
    "            \n",
    "            # Get cluster samples\n",
    "            cluster_samples = np.where(sample_to_cluster == cluster_idx)[0]\n",
    "            \n",
    "            # Plot all points in gray\n",
    "            ax_pca.scatter(X_pca[:, 0], X_pca[:, 1], c='lightgray', alpha=0.3, s=10)\n",
    "            \n",
    "            # Highlight this cluster\n",
    "            ax_pca.scatter(\n",
    "                X_pca[cluster_samples, 0], \n",
    "                X_pca[cluster_samples, 1], \n",
    "                c=colors[i % len(colors)],\n",
    "                alpha=0.7, \n",
    "                s=30,\n",
    "                edgecolor='black'\n",
    "            )\n",
    "            \n",
    "            ax_pca.set_title(f\"PCA Visualization\", fontsize=12)\n",
    "            ax_pca.set_xticks([])\n",
    "            ax_pca.set_yticks([])\n",
    "        \n",
    "        # Add a circular indicator for quality\n",
    "        if cohesion is not None:\n",
    "            quality_score = (cohesion + (stability if 'stability' in locals() else 0.5)) / 2\n",
    "            \n",
    "            # Create quality indicator\n",
    "            ax_quality = plt.axes([0.45, 0.25 + i*0.2, 0.1, 0.15], polar=True)\n",
    "            \n",
    "            # Create a gauge chart\n",
    "            theta = np.linspace(0, 2*np.pi, 100)\n",
    "            r = np.ones_like(theta)\n",
    "            \n",
    "            # Background circle (gray)\n",
    "            ax_quality.fill_between(theta, 0, r, color='lightgray', alpha=0.3)\n",
    "            \n",
    "            # Foreground circle (colored by quality)\n",
    "            quality_theta = np.linspace(0, 2*np.pi*quality_score, 100)\n",
    "            quality_r = np.ones_like(quality_theta)\n",
    "            \n",
    "            # Choose color based on quality\n",
    "            if quality_score > 0.8:\n",
    "                quality_color = 'green'\n",
    "                quality_label = 'Excellent'\n",
    "            elif quality_score > 0.6:\n",
    "                quality_color = 'yellowgreen'\n",
    "                quality_label = 'Good'\n",
    "            elif quality_score > 0.4:\n",
    "                quality_color = 'orange'\n",
    "                quality_label = 'Moderate'\n",
    "            else:\n",
    "                quality_color = 'red'\n",
    "                quality_label = 'Poor'\n",
    "                \n",
    "            ax_quality.fill_between(quality_theta, 0, quality_r, color=quality_color, alpha=0.7)\n",
    "            \n",
    "            # Remove ticks and spines\n",
    "            ax_quality.set_xticks([])\n",
    "            ax_quality.set_yticks([])\n",
    "            ax_quality.spines['polar'].set_visible(False)\n",
    "            \n",
    "            # Add quality label\n",
    "            ax_quality.text(\n",
    "                0, 0,\n",
    "                f\"{quality_score:.2f}\\n{quality_label}\",\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                fontsize=10,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "            \n",
    "            ax_quality.set_title(\"Quality\", fontsize=12)\n",
    "        \n",
    "    plt.suptitle(\"Top 5 Best Clusters - Detailed Profiles\", fontsize=20, y=0.98)\n",
    "    plt.savefig(f\"{output_dir}/8_top5_cluster_profiles.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # =============== 9. Generate a summary HTML report ===============\n",
    "    try:\n",
    "        # Create an HTML summary that links to all the images\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Best Clusters Analysis</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
    "                h1, h2 {{ color: #333366; }}\n",
    "                .container {{ max-width: 1200px; margin: auto; background-color: white; padding: 20px; border-radius: 10px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }}\n",
    "                .img-container {{ margin: 20px 0; text-align: center; }}\n",
    "                img {{ max-width: 100%; border: 1px solid #ddd; border-radius: 5px; }}\n",
    "                .summary {{ background-color: #e6f7ff; padding: 15px; border-radius: 5px; margin-bottom: 20px; }}\n",
    "                table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}\n",
    "                th, td {{ padding: 10px; border: 1px solid #ddd; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "                a {{ color: #0066cc; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>Best Clusters Analysis</h1>\n",
    "                \n",
    "                <div class=\"summary\">\n",
    "                    <h2>Summary</h2>\n",
    "                    <p><strong>Total Samples:</strong> {total_samples}</p>\n",
    "                    <p><strong>Number of Clusters:</strong> {n_clusters}</p>\n",
    "                    <p><strong>Number of Best Clusters Analyzed:</strong> {len(best_clusters_idx)}</p>\n",
    "                    <p><strong>Percentage of Samples in Best Clusters:</strong> {best_percent:.2f}%</p>\n",
    "                </div>\n",
    "                \n",
    "                <h2>Visualizations</h2>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add all the images\n",
    "        images = [\n",
    "            (\"1_best_clusters_overview.png\", \"Best Clusters Overview\"),\n",
    "            (\"2_best_clusters_visualization.png\", \"Best Clusters Visualization\"),\n",
    "            (\"3_feature_analysis.png\", \"Feature Analysis\"),\n",
    "            (\"4_feature_distributions.png\", \"Feature Distributions\"),\n",
    "            (\"5_cluster_similarity.png\", \"Cluster Similarity Analysis\"),\n",
    "            (\"6_silhouette_analysis.png\", \"Silhouette Analysis\"),\n",
    "            (\"7_stability_analysis.png\", \"Stability Analysis\"),\n",
    "            (\"8_top5_cluster_profiles.png\", \"Top 5 Cluster Profiles\")\n",
    "        ]\n",
    "        \n",
    "        for img_file, img_title in images:\n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"img-container\">\n",
    "                    <h3>{img_title}</h3>\n",
    "                    <a href=\"{img_file}\" target=\"_blank\">\n",
    "                        <img src=\"{img_file}\" alt=\"{img_title}\">\n",
    "                    </a>\n",
    "                    <p><a href=\"{img_file}\" download>Download this image</a></p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Add top clusters table\n",
    "        html_content += \"\"\"\n",
    "                <h2>Best Clusters Summary</h2>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Cluster ID</th>\n",
    "                        <th>Size</th>\n",
    "                        <th>% of Total</th>\n",
    "        \"\"\"\n",
    "        \n",
    "        if similarity_matrix is not None:\n",
    "            html_content += \"\"\"\n",
    "                        <th>Cohesion</th>\n",
    "            \"\"\"\n",
    "            \n",
    "        if 'stability_scores' in locals():\n",
    "            html_content += \"\"\"\n",
    "                        <th>Stability</th>\n",
    "            \"\"\"\n",
    "            \n",
    "        html_content += \"\"\"\n",
    "                    </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add data for best clusters\n",
    "        for cluster_idx in best_clusters_idx:\n",
    "            cluster_size = densities[cluster_idx]\n",
    "            percentage = (cluster_size / total_samples) * 100\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td>{cluster_idx}</td>\n",
    "                        <td>{cluster_size}</td>\n",
    "                        <td>{percentage:.2f}%</td>\n",
    "            \"\"\"\n",
    "            \n",
    "            if similarity_matrix is not None:\n",
    "                html_content += f\"\"\"\n",
    "                        <td>{cohesion_scores[cluster_idx]:.4f}</td>\n",
    "                \"\"\"\n",
    "                \n",
    "            if 'stability_scores' in locals() and cluster_idx in stability_scores and stability_scores[cluster_idx]:\n",
    "                stability = np.mean(stability_scores[cluster_idx])\n",
    "                html_content += f\"\"\"\n",
    "                        <td>{stability:.4f}</td>\n",
    "                \"\"\"\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "                    </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "                </table>\n",
    "                \n",
    "                <h2>Conclusion</h2>\n",
    "                <p>\n",
    "                    This analysis highlights the best clusters identified by your algorithm, showcasing their \n",
    "                    distinctive features, internal cohesion, and stability. The visualizations demonstrate how \n",
    "                    well-separated these clusters are and which features contribute most to their formation.\n",
    "                </p>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Close tags\n",
    "        html_content += \"\"\"\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Write the HTML file\n",
    "        with open(f\"{output_dir}/best_clusters_report.html\", \"w\") as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"HTML report generated: {output_dir}/best_clusters_report.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating HTML report: {e}\")\n",
    "    \n",
    "    print(f\"All best cluster visualizations saved to {output_dir}/\")\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_best_cluster_analysis(\n",
    "    clusters=clusters, \n",
    "    densities=densities,\n",
    "    features_df=features_df, \n",
    "    similarity_matrix=similarity,  # Pass the similarity matrix\n",
    "    # labels=labels,  # Optional\n",
    "    output_dir=\"best_cluster_analysis\"  # Output directory\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
